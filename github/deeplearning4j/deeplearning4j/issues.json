{
  "data":{
    "repository":{
      "issues":{
        "nodes":[
          {
            "number":1453,
            "title":"Add notes on *too much* regularization (l1, l2, dropout) in troubleshooting guide",
            "bodyText":"See title.\nhttp://deeplearning4j.org/troubleshootingneuralnets",
            "url":"https://github.com/deeplearning4j/deeplearning4j/issues/1453"
          },
          {
            "number":1602,
            "title":"Documentation request: List of layers available and how to use",
            "bodyText":"Looking for a cheat sheet to help understand the various types of layers (ConvolutionLayer, SubsamplingLayer..) in a multilayer Neural network , and rules for using them (eg builder errors when one illegally follows another)",
            "url":"https://github.com/deeplearning4j/deeplearning4j/issues/1602"
          },
          {
            "number":1831,
            "title":"Apache beam integration",
            "bodyText":"Interest has been expressed in apache flink integration.\nInstead what we may do is look at doing beam integration and swap out our spark integration.\nProgramming against dataflow allows us to have any backend we want and we can use 1 api for multiple streaming frameworks.",
            "url":"https://github.com/deeplearning4j/deeplearning4j/issues/1831"
          },
          {
            "number":2671,
            "title":"Document learning rate schedules",
            "bodyText":"Specifically:\n(a) what LR schedules are available, and how to use them\nhttps://github.com/deeplearning4j/deeplearning4j/blob/91a481ae8f5bcb4c9ff3463c1bba2df69d7325d2/deeplearning4j-nn/src/main/java/org/deeplearning4j/nn/conf/LearningRatePolicy.java\nand,\n(b) What the actual mathematical form of these are (and ideally: with example graphs for common configurations)",
            "url":"https://github.com/deeplearning4j/deeplearning4j/issues/2671"
          },
          {
            "number":2769,
            "title":"Documentation for ParagraphVectors learning algorithms",
            "bodyText":"I found it pretty confusing trying to figure out what the parameters elementsLearningAlgorithm and sequenceLearningAlgorithm mean for ParagraphVectors. First, the web page at https://deeplearning4j.org/word2vec says:\n\nor using a word to predict a target context, which is called skip-gram. We use the latter method because it produces more accurate results on large datasets.\n\nwhich lead me to believe that your implementation of ParagraphVectors was always DM. Was additionally confused because the JavaDoc contains those 2 parameters but I couldn't figure out what it would mean to set them both to different values.\nHad a chat on gitter which cleared things up but Adam Gibson suggested I file a bug in addition so the docs will get updated.",
            "url":"https://github.com/deeplearning4j/deeplearning4j/issues/2769"
          },
          {
            "number":2770,
            "title":"Documentation for ParagraphVectors learning rate",
            "bodyText":"Most of your algorithms allow one to specify pretty fine grained learning parameters (e.g. LearningRatePolicy) but ParagraphVectors only allows you to specify a learning rate and a minimum learning rate. It's not entirely clear what kind of learning rate schedule this corresponds to. As far as I know there aren't any other learning rate tuning options.",
            "url":"https://github.com/deeplearning4j/deeplearning4j/issues/2770"
          },
          {
            "number":2772,
            "title":"Allow pre-tokenized text with Word2Vec/ParagraphVectors",
            "bodyText":"In our situation I have some text that I've already tokenized (to do part of speech tagging, etc.) Unfortunately the implementation of ParagraphVectors requires that you provide your text as a String and that you provide a TokenizerFactory. As a work-around I simply concatenate my tokens with spaces and then use the default tokenizer. This works but is ugly.\nGitter conversation about this:\nhttps://gitter.im/deeplearning4j/deeplearning4j?at=588a8f39c0f28dd8624a77ad",
            "url":"https://github.com/deeplearning4j/deeplearning4j/issues/2772"
          },
          {
            "number":2774,
            "title":"Page on important Neural Networks",
            "bodyText":"Not sure if we have this or not, if not we should build one, also wikipedia could use similar content\nTitle Significant Neural Networks\nHistory and Background of the image prediction networks\nImageNet:\nStanford University maintains and develops ImageNet. ImageNet is a labelled dataset of images. As of January 2017, ImageNet contained 14,197,122 images.\nImageNet Challenge: Each year starting in 2010 research teams submit programs to the Imagenet Large Scale Visual Recognition Challenge (ILSVCR) .\nIn recent years deep convolutional networks have taken the lead in accuracy at the challenge.\nSome Neural Networks have therefore become widely known and studied in the deeplearning community. If you are just getting started you should be aware of the following Neural networks.\nVGG16\nVGG19\nGoogleNet\nLenet\nInception\nVGG Models.\nVGG models are developed by the University of Oxford's Visual Geometry Group.\nhttp://www.robots.ox.ac.uk/~vgg/research/very_deep/\nIn ImageNet ILSVRC-2014 the VGG group had two Neural Networks that performed very well in the localization and classification challenge. The widely available configurations for the VGG models are VGG-16 and VGG-19, they are named for the number of Hidden Layers.\nThe neural networks where tuned slightly afterwards and both the model configurations and the trained weights have been made available.\nhttps://gist.github.com/ksimonyan/211839e770f7b538e2d8#file-readme-md\nhttps://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md\nGoogleNet\nInception",
            "url":"https://github.com/deeplearning4j/deeplearning4j/issues/2774"
          },
          {
            "number":2896,
            "title":"Provide an example about generating vectors from provided sentences",
            "bodyText":"I'm trying to find a way to train W2V model with provided tokens. I've already tokenized and lemmatized tokens, some of them are n-grams such as: \"mc Donald's\" \"LA Lakers\" \"European Union\" and I've them in a data structure as: List<List> sentences\nAnd I want to add them to W2V as:\n// read, sentence extract, lemmatize with different ways and generate List<List> sentences\nfor (List sentenceTokens : sentences) {\n// provide a way to feed W2V\n}\nIs it possible to provide a sample project for this logic so I can find neighbours such as W2V.wordsNearest(\"LA Lakers\", 10);?",
            "url":"https://github.com/deeplearning4j/deeplearning4j/issues/2896"
          },
          {
            "number":3063,
            "title":"incremental intent classification learning with pagraphvectors",
            "bodyText":"I'm working a conversation platform like https://api.ai or https://www.ibmwatsonconversation.com, whenever user creates an intent, he/she will provide some user intent examples(like 5 or more)，if i only use the provided user intent examples, pargraphvectors can not get enough data for training and causes a bad accuracy, besides I found some only-exists-in-one-sentence words get an almost zero vector.\nBecause of that, I want to provide paragraph a basic wordvectors(trained from a large corpus)，and let the paragraphvectors learn base on this basic wordvectors. However, I found that once wordvectors provided through useExistingWordVectors, the trainElementsRepresentation(false) is called, so no new vocabword will be learned from new data incrementally, and this will cause bad classification or even cannot inferVector because  tokenized rawText can not match any word in the vocabcache.\nAnd another practical engineering problem for memory usage with the above incremental method, even though the paragraphvectors can do incemental learning, the basic wordvectors is normally very large. As a conversation platform, i need to provide every agent/workspace a separate model(paragraphvectors),  this will cause too much memory usage and  will need much machine instances.  My solution(not verifed) based on inremental-learning-support-paragraphvectors is to wrapper a wordvectors which hold two separate word2vec instances, one from user examples and the other form large corpus, and the method calls on the large-corpus-word2vec is remote from another centralized and distributed server",
            "url":"https://github.com/deeplearning4j/deeplearning4j/issues/3063"
          },
          {
            "number":3193,
            "title":"Toilet the Reuters News Dataset classes",
            "bodyText":"Issue Description\nThe Reuters news dataset is a classic dataset used for testing many algorithms :\nhttps://archive.ics.uci.edu/ml/datasets/Reuters-21578+Text+Categorization+Collection\nWe have in our codebase some skeleton classes left out of an attempt to integrate this into our testing datasets (supported by default, like the Iris dataset):\nhttps://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/datasets/iterator/ReutersNewsGroupsDataSetIterator.java\nhttps://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-nlp-parent/deeplearning4j-nlp/src/main/java/org/deeplearning4j/datasets/loader/ReutersNewsGroupsLoader.java\nIt's incorrectly labeled \"reuters newsgroup\" (the reuters news dataset and the 20 newsgroups are 2 distinct datasets, both classics, this is the former).\nThose classes should be cleaned up and be brought ini line with the functionality of :\nhttps://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-core/src/main/java/org/deeplearning4j/datasets/fetchers/IrisDataFetcher.java\nhttps://github.com/deeplearning4j/deeplearning4j/blob/master/deeplearning4j-core/src/main/java/org/deeplearning4j/datasets/iterator/impl/IrisDataSetIterator.java\nVersion Information\nmaster at f21107e",
            "url":"https://github.com/deeplearning4j/deeplearning4j/issues/3193"
          }
        ]
      }
    }
  }
}