{
  "organization" : "microsoft",
  "repository" : "sql-spark-connector",
  "creationDate" : 1630311421000,
  "githubStatus" : {
    "Ok" : {
      "updateDate" : 1730788109999
    }
  },
  "githubInfo" : {
    "homepage" : null,
    "description" : "Apache Spark Connector for SQL Server and Azure SQL",
    "logo" : "https://avatars.githubusercontent.com/u/6154722?v=4",
    "stars" : 273,
    "forks" : 118,
    "watchers" : 35,
    "issues" : 59,
    "creationDate" : 1591138457000,
    "readme" : "<div id=\"readme\" class=\"md\" data-path=\"README.md\"><article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><p align=\"center\" dir=\"auto\">\n  <a target=\"_blank\" rel=\"noopener noreferrer\" href=\"sql-spark-connector-icon.svg\"><img src=\"sql-spark-connector-icon.svg\" alt=\"Apache Spark Connector for SQL Server and Azure SQL\" width=\"150\" style=\"max-width: 100%;\"></a>\n</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h1 class=\"heading-element\" dir=\"auto\">Apache Spark Connector for SQL Server and Azure SQL</h1><a id=\"user-content-apache-spark-connector-for-sql-server-and-azure-sql\" class=\"anchor\" aria-label=\"Permalink: Apache Spark Connector for SQL Server and Azure SQL\" href=\"#apache-spark-connector-for-sql-server-and-azure-sql\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">Born out of Microsoft’s SQL Server Big Data Clusters investments, the Apache Spark Connector for SQL Server and Azure SQL is a high-performance connector that enables you to use transactional data in big data analytics and persists results for ad-hoc queries or reporting. The connector allows you to use any SQL database, on-premises or in the cloud, as an input data source or output data sink for Spark jobs.</p>\n<p dir=\"auto\">This library contains the source code for the Apache Spark Connector for SQL Server and Azure SQL.</p>\n<p dir=\"auto\"><a href=\"https://spark.apache.org/\" rel=\"nofollow\">Apache Spark</a> is a unified analytics engine for large-scale data processing.</p>\n<p dir=\"auto\">There are three version sets of the connector available through Maven, a 2.4.x, a 3.0.x and a 3.1.x compatible version. All versions can be found <a href=\"https://search.maven.org/search?q=spark-mssql-connector\" rel=\"nofollow\">here</a> and can be imported using the coordinates below:</p>\n<markdown-accessiblity-table><table>\n<thead>\n<tr>\n<th>Connector</th>\n<th>Maven Coordinate</th>\n<th>Scala Version</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Spark 2.4.x compatible connector</td>\n<td><code>com.microsoft.azure:spark-mssql-connector:1.0.2</code></td>\n<td>2.11</td>\n</tr>\n<tr>\n<td>Spark 3.0.x compatible connector</td>\n<td><code>com.microsoft.azure:spark-mssql-connector_2.12:1.1.0</code></td>\n<td>2.12</td>\n</tr>\n<tr>\n<td>Spark 3.1.x compatible connector</td>\n<td><code>com.microsoft.azure:spark-mssql-connector_2.12:1.2.0</code></td>\n<td>2.12</td>\n</tr>\n<tr>\n<td>Spark 3.3.x compatible connector</td>\n<td><code>com.microsoft.azure:spark-mssql-connector_2.12:1.3.0</code></td>\n<td>2.12</td>\n</tr>\n<tr>\n<td>Spark 3.4.x compatible connector</td>\n<td><code>com.microsoft.azure:spark-mssql-connector_2.12:1.4.0</code></td>\n<td>2.12</td>\n</tr>\n</tbody>\n</table></markdown-accessiblity-table>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Current Releases</h2><a id=\"user-content-current-releases\" class=\"anchor\" aria-label=\"Permalink: Current Releases\" href=\"#current-releases\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">The latest Spark 2.4.x compatible connector is on v1.0.2.</p>\n<p dir=\"auto\">The latest Spark 3.0.x compatible connector is on v1.1.0.</p>\n<p dir=\"auto\">The latest Spark 3.1.x compatible connector is on v1.2.0.</p>\n<p dir=\"auto\">For main changes from previous releases and known issues please refer to <a href=\"docs/CHANGELIST.md\">CHANGELIST</a></p>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Supported Features</h2><a id=\"user-content-supported-features\" class=\"anchor\" aria-label=\"Permalink: Supported Features\" href=\"#supported-features\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<ul dir=\"auto\">\n<li>Support for all Spark bindings (Scala, Python, R)</li>\n<li>Basic authentication and Active Directory (AD) Key Tab support</li>\n<li>Reordered DataFrame write support</li>\n<li>Support for write to SQL Server Single instance and Data Pool in SQL Server Big Data Clusters</li>\n<li>Reliable connector support for Sql Server Single Instance</li>\n</ul>\n<markdown-accessiblity-table><table>\n<thead>\n<tr>\n<th>Component</th>\n<th>Versions Supported</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Apache Spark</td>\n<td>2.4.x, 3.0.x, 3.1.x, 3.3.x</td>\n</tr>\n<tr>\n<td>Scala</td>\n<td>2.11, 2.12</td>\n</tr>\n<tr>\n<td>Microsoft JDBC Driver for SQL Server</td>\n<td>8.4.1</td>\n</tr>\n<tr>\n<td>Microsoft SQL Server</td>\n<td>SQL Server 2008 or later</td>\n</tr>\n<tr>\n<td>Azure SQL Databases</td>\n<td>Supported</td>\n</tr>\n</tbody>\n</table></markdown-accessiblity-table>\n<p dir=\"auto\"><em>Note: Azure Synapse (Azure SQL DW) use is not tested with this connector. While it may work, there may be unintended consequences.</em></p>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Supported Options</h3><a id=\"user-content-supported-options\" class=\"anchor\" aria-label=\"Permalink: Supported Options\" href=\"#supported-options\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">The Apache Spark Connector for SQL Server and Azure SQL supports the options defined here: <a href=\"https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html\" rel=\"nofollow\">SQL DataSource JDBC</a></p>\n<p dir=\"auto\">In addition following options are supported</p>\n<markdown-accessiblity-table><table>\n<thead>\n<tr>\n<th>Option</th>\n<th>Default</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>reliabilityLevel</td>\n<td>\"BEST_EFFORT\"</td>\n<td>\"BEST_EFFORT\" or \"NO_DUPLICATES\". \"NO_DUPLICATES\" implements an reliable insert in executor restart scenarios</td>\n</tr>\n<tr>\n<td>dataPoolDataSource</td>\n<td>none</td>\n<td>none implies the value is not set and the connector should write to SQl Server Single Instance. Set this value to data source name to write a Data Pool Table in Big Data Cluster</td>\n</tr>\n<tr>\n<td>isolationLevel</td>\n<td>\"READ_COMMITTED\"</td>\n<td>Specify the isolation level</td>\n</tr>\n<tr>\n<td>tableLock</td>\n<td>\"false\"</td>\n<td>Implements an insert with TABLOCK option to improve write performance</td>\n</tr>\n<tr>\n<td>schemaCheckEnabled</td>\n<td>\"true\"</td>\n<td>Disables strict dataframe and sql table schema check when set to false</td>\n</tr>\n</tbody>\n</table></markdown-accessiblity-table>\n<p dir=\"auto\">Other <a href=\"https://docs.microsoft.com/en-us/sql/connect/jdbc/using-bulk-copy-with-the-jdbc-driver?view=sql-server-2017#sqlserverbulkcopyoptions\" rel=\"nofollow\">Bulk api options</a> can be set as options on the dataframe and will be passed to bulkcopy apis on write</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Performance comparison</h2><a id=\"user-content-performance-comparison\" class=\"anchor\" aria-label=\"Permalink: Performance comparison\" href=\"#performance-comparison\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">Apache Spark Connector for SQL Server and Azure SQL is up to 15x faster than generic JDBC connector for writing to SQL Server. Note performance characteristics vary on type, volume of data,  options used and may show run to run variations. The following performance results are the time taken to overwrite a sql table with 143.9M rows in a spark dataframe. The spark dataframe is constructed by reading store_sales HDFS table generated using <a href=\"https://github.com/databricks/spark-sql-perf\">spark TPCDS Benchmark</a>. Time to read store_sales to dataframe is excluded. The results are averaged over 3 runs.\n<em>Note: The following results were achieved using the Apache Spark 2.4.5 compatible connector. These numbers are not a guarantee of performance.</em></p>\n<markdown-accessiblity-table><table>\n<thead>\n<tr>\n<th>Connector Type</th>\n<th>Options</th>\n<th>Description</th>\n<th>Time to write</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>JDBCConnector</td>\n<td>Default</td>\n<td>Generic JDBC connector with default options</td>\n<td>1385s</td>\n</tr>\n<tr>\n<td>sql-spark-connector</td>\n<td>BEST_EFFORT</td>\n<td>Best effort sql-spark-connector  with default options</td>\n<td>580s</td>\n</tr>\n<tr>\n<td>sql-spark-connector</td>\n<td>NO_DUPLICATES</td>\n<td>Reliable sql-spark-connector</td>\n<td>709s</td>\n</tr>\n<tr>\n<td>sql-spark-connector</td>\n<td>BEST_EFFORT + tabLock=true</td>\n<td>Best effort sql-spark-connector with table lock enabled</td>\n<td>72s</td>\n</tr>\n<tr>\n<td>sql-spark-connector</td>\n<td>NO_DUPLICATES + tabLock=true</td>\n<td>Reliable sql-spark-connector with table lock enabled</td>\n<td>198s</td>\n</tr>\n</tbody>\n</table></markdown-accessiblity-table>\n<p dir=\"auto\">Config</p>\n<ul dir=\"auto\">\n<li>Spark config : <code>num_executors = 20</code>, <code>executor_memory = '1664m'</code>, <code>executor_cores = 2</code></li>\n<li>Data Gen config : <code>scale_factor=50</code>, <code>partitioned_tables=true</code></li>\n<li>Data file Store_sales with number of of rows 143,997,590</li>\n</ul>\n<p dir=\"auto\">Environment</p>\n<ul dir=\"auto\">\n<li><a href=\"https://docs.microsoft.com/en-us/sql/big-data-cluster/release-notes-big-data-cluster?view=sql-server-ver15\" rel=\"nofollow\">SQL Server Big Data Cluster</a> CU5</li>\n<li>Master + 6 nodes</li>\n<li>Each node gen 5 server, 512GB Ram, 4TB NVM per node, NIC 10GB</li>\n</ul>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Commonly Faced Issues</h2><a id=\"user-content-commonly-faced-issues\" class=\"anchor\" aria-label=\"Permalink: Commonly Faced Issues\" href=\"#commonly-faced-issues\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\"><code>java.lang.NoClassDefFoundError: com/microsoft/aad/adal4j/AuthenticationException</code></h3><a id=\"user-content-javalangnoclassdeffounderror-commicrosoftaadadal4jauthenticationexception\" class=\"anchor\" aria-label=\"Permalink: java.lang.NoClassDefFoundError: com/microsoft/aad/adal4j/AuthenticationException\" href=\"#javalangnoclassdeffounderror-commicrosoftaadadal4jauthenticationexception\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">This issue arises from using an older version of the mssql driver (which is now included in this connector) in your hadoop environment. If you are coming from using the previous Azure SQL Connector and have manually installed drivers onto that cluster for AAD compatibility, you will need to remove those drivers.</p>\n<p dir=\"auto\">Steps to fix the issue:</p>\n<ol dir=\"auto\">\n<li>If you are using a generic Hadoop environment, check and remove the mssql jar: <code>rm $HADOOP_HOME/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar</code>.\nIf you are using Databricks, add a global or cluster init script to remove old versions of the mssql driver from the <code>/databricks/jars</code> folder, or add this line to an existing script: <code>rm /databricks/jars/*mssql*</code></li>\n<li>Add the <code>adal4j</code> and <code>mssql</code> packages, I used Maven, but anyway should work. DO NOT install the SQL spark connector this way.</li>\n<li>Add the driver class to your connection configuration:</li>\n</ol>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"connectionProperties = {\n  &quot;Driver&quot;: &quot;com.microsoft.sqlserver.jdbc.SQLServerDriver&quot;\n}\"><pre class=\"notranslate\"><code>connectionProperties = {\n  \"Driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n}\n</code></pre></div>\n<p dir=\"auto\">For more information and explanation, visit the closed <a href=\"https://github.com/microsoft/sql-spark-connector/issues/26\" data-hovercard-type=\"issue\" data-hovercard-url=\"/microsoft/sql-spark-connector/issues/26/hovercard\">issue</a>.</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Get Started</h2><a id=\"user-content-get-started\" class=\"anchor\" aria-label=\"Permalink: Get Started\" href=\"#get-started\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">The Apache Spark Connector for SQL Server and Azure SQL is based on the Spark DataSourceV1 API and SQL Server Bulk API and uses the same interface as the built-in JDBC Spark-SQL connector. This allows you to easily integrate the connector and migrate your existing Spark jobs by simply updating the format parameter with <code>com.microsoft.sqlserver.jdbc.spark</code>.</p>\n<p dir=\"auto\">To include the connector in your projects download this repository and build the jar using SBT.</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Migrating from Legacy Azure SQL Connector for Spark</h3><a id=\"user-content-migrating-from-legacy-azure-sql-connector-for-spark\" class=\"anchor\" aria-label=\"Permalink: Migrating from Legacy Azure SQL Connector for Spark\" href=\"#migrating-from-legacy-azure-sql-connector-for-spark\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h4 class=\"heading-element\" dir=\"auto\">Receiving <code>java.lang.NoClassDefFoundError</code> when trying to use the new connector with Azure Databricks?</h4><a id=\"user-content-receiving-javalangnoclassdeffounderror-when-trying-to-use-the-new-connector-with-azure-databricks\" class=\"anchor\" aria-label=\"Permalink: Receiving java.lang.NoClassDefFoundError when trying to use the new connector with Azure Databricks?\" href=\"#receiving-javalangnoclassdeffounderror-when-trying-to-use-the-new-connector-with-azure-databricks\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">If you are migrating from the previous Azure SQL Connector for Spark and have manually installed drivers onto that cluster for AAD compatibility, you will most likely need to remove those custom drivers, restore the previous drivers that ship by default with Databricks, uninstall the previous connector, and restart your cluster.  You may be better off spinning up a new cluster.</p>\n<p dir=\"auto\">With this new connector, you should be able to simply install onto a cluster (new or existing cluster that hasn't had its drivers modified) or a cluster which previously used modified drivers for the older Azure SQL Connector for Spark provided the modified drivers were removed and the previous default drivers restored.</p>\n<p dir=\"auto\">See <a href=\"https://github.com/microsoft/sql-spark-connector/issues/26\" data-hovercard-type=\"issue\" data-hovercard-url=\"/microsoft/sql-spark-connector/issues/26/hovercard\">Issue #26</a> for more details.</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Executing custom SQL through the connector</h3><a id=\"user-content-executing-custom-sql-through-the-connector\" class=\"anchor\" aria-label=\"Permalink: Executing custom SQL through the connector\" href=\"#executing-custom-sql-through-the-connector\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">The previous Azure SQL Connector for Spark provided the ability to execute custom SQL code like DML or DDL statements through the connector. This functionality is out-of-scope of this connector since it is based on the DataSource APIs. This functionality is readily provided by libraries like pyodbc or you can use the standard java sql interfaces as well.</p>\n<p dir=\"auto\">You can read the closed issue and view community provided alternatives in <a href=\"https://github.com/microsoft/sql-spark-connector/issues/21\" data-hovercard-type=\"issue\" data-hovercard-url=\"/microsoft/sql-spark-connector/issues/21/hovercard\">Issue #21</a>.</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Write to a new SQL Table</h3><a id=\"user-content-write-to-a-new-sql-table\" class=\"anchor\" aria-label=\"Permalink: Write to a new SQL Table\" href=\"#write-to-a-new-sql-table\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\"><g-emoji class=\"g-emoji\" alias=\"warning\">⚠️</g-emoji> <strong>Important: using the <code>overwrite</code> mode will first DROP the table if it already exists in the database by default. Please use this option with due care to avoid unexpected data loss!</strong></p>\n<p dir=\"auto\"><g-emoji class=\"g-emoji\" alias=\"warning\">⚠️</g-emoji> <strong>When using mode <code>overwrite</code> if you do not use the option <code>truncate</code>, on recreation of the table indexes will be lost. For example a columnstore table would now be a heap. If you want to maintain existing indexing please also specify option <code>truncate</code> with value true. i.e <code>.option(\"truncate\",true)</code></strong></p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"server_name = &quot;jdbc:sqlserver://{SERVER_ADDR}&quot;\ndatabase_name = &quot;database_name&quot;\nurl = server_name + &quot;;&quot; + &quot;databaseName=&quot; + database_name + &quot;;&quot;\n\ntable_name = &quot;table_name&quot;\nusername = &quot;username&quot;\npassword = &quot;password123!#&quot; # Please specify password here\n\ntry:\n  df.write \\\n    .format(&quot;com.microsoft.sqlserver.jdbc.spark&quot;) \\\n    .mode(&quot;overwrite&quot;) \\\n    .option(&quot;url&quot;, url) \\\n    .option(&quot;dbtable&quot;, table_name) \\\n    .option(&quot;user&quot;, username) \\\n    .option(&quot;password&quot;, password) \\\n    .save()\nexcept ValueError as error :\n    print(&quot;Connector write failed&quot;, error)\"><pre><span class=\"pl-s1\">server_name</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">\"jdbc:sqlserver://{SERVER_ADDR}\"</span>\n<span class=\"pl-s1\">database_name</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">\"database_name\"</span>\n<span class=\"pl-s1\">url</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">server_name</span> <span class=\"pl-c1\">+</span> <span class=\"pl-s\">\";\"</span> <span class=\"pl-c1\">+</span> <span class=\"pl-s\">\"databaseName=\"</span> <span class=\"pl-c1\">+</span> <span class=\"pl-s1\">database_name</span> <span class=\"pl-c1\">+</span> <span class=\"pl-s\">\";\"</span>\n\n<span class=\"pl-s1\">table_name</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">\"table_name\"</span>\n<span class=\"pl-s1\">username</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">\"username\"</span>\n<span class=\"pl-s1\">password</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">\"password123!#\"</span> <span class=\"pl-c\"># Please specify password here</span>\n\n<span class=\"pl-k\">try</span>:\n  <span class=\"pl-s1\">df</span>.<span class=\"pl-s1\">write</span> \\\n    .<span class=\"pl-en\">format</span>(<span class=\"pl-s\">\"com.microsoft.sqlserver.jdbc.spark\"</span>) \\\n    .<span class=\"pl-en\">mode</span>(<span class=\"pl-s\">\"overwrite\"</span>) \\\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"url\"</span>, <span class=\"pl-s1\">url</span>) \\\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"dbtable\"</span>, <span class=\"pl-s1\">table_name</span>) \\\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"user\"</span>, <span class=\"pl-s1\">username</span>) \\\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"password\"</span>, <span class=\"pl-s1\">password</span>) \\\n    .<span class=\"pl-en\">save</span>()\n<span class=\"pl-k\">except</span> <span class=\"pl-v\">ValueError</span> <span class=\"pl-k\">as</span> <span class=\"pl-s1\">error</span> :\n    <span class=\"pl-en\">print</span>(<span class=\"pl-s\">\"Connector write failed\"</span>, <span class=\"pl-s1\">error</span>)</pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Append to SQL Table</h3><a id=\"user-content-append-to-sql-table\" class=\"anchor\" aria-label=\"Permalink: Append to SQL Table\" href=\"#append-to-sql-table\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"try:\n  df.write \\\n    .format(&quot;com.microsoft.sqlserver.jdbc.spark&quot;) \\\n    .mode(&quot;append&quot;) \\\n    .option(&quot;url&quot;, url) \\\n    .option(&quot;dbtable&quot;, table_name) \\\n    .option(&quot;user&quot;, username) \\\n    .option(&quot;password&quot;, password) \\\n    .save()\nexcept ValueError as error :\n    print(&quot;Connector write failed&quot;, error)\"><pre><span class=\"pl-k\">try</span>:\n  <span class=\"pl-s1\">df</span>.<span class=\"pl-s1\">write</span> \\\n    .<span class=\"pl-en\">format</span>(<span class=\"pl-s\">\"com.microsoft.sqlserver.jdbc.spark\"</span>) \\\n    .<span class=\"pl-en\">mode</span>(<span class=\"pl-s\">\"append\"</span>) \\\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"url\"</span>, <span class=\"pl-s1\">url</span>) \\\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"dbtable\"</span>, <span class=\"pl-s1\">table_name</span>) \\\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"user\"</span>, <span class=\"pl-s1\">username</span>) \\\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"password\"</span>, <span class=\"pl-s1\">password</span>) \\\n    .<span class=\"pl-en\">save</span>()\n<span class=\"pl-k\">except</span> <span class=\"pl-v\">ValueError</span> <span class=\"pl-k\">as</span> <span class=\"pl-s1\">error</span> :\n    <span class=\"pl-en\">print</span>(<span class=\"pl-s\">\"Connector write failed\"</span>, <span class=\"pl-s1\">error</span>)</pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Specifying the isolation level</h3><a id=\"user-content-specifying-the-isolation-level\" class=\"anchor\" aria-label=\"Permalink: Specifying the isolation level\" href=\"#specifying-the-isolation-level\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">This connector by default uses <code>READ_COMMITTED</code> isolation level when performing the bulk insert into the database. If you wish to override this to another isolation level, please use the <code>mssqlIsolationLevel</code> option as shown below.</p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"    .option(&quot;mssqlIsolationLevel&quot;, &quot;READ_UNCOMMITTED&quot;) \\\"><pre>    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"mssqlIsolationLevel\"</span>, <span class=\"pl-s\">\"READ_UNCOMMITTED\"</span>) \\</pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Read from SQL Table</h3><a id=\"user-content-read-from-sql-table\" class=\"anchor\" aria-label=\"Permalink: Read from SQL Table\" href=\"#read-from-sql-table\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"jdbcDF = spark.read \\\n        .format(&quot;com.microsoft.sqlserver.jdbc.spark&quot;) \\\n        .option(&quot;url&quot;, url) \\\n        .option(&quot;dbtable&quot;, table_name) \\\n        .option(&quot;user&quot;, username) \\\n        .option(&quot;password&quot;, password).load()\"><pre><span class=\"pl-s1\">jdbcDF</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">spark</span>.<span class=\"pl-s1\">read</span> \\\n        .<span class=\"pl-en\">format</span>(<span class=\"pl-s\">\"com.microsoft.sqlserver.jdbc.spark\"</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"url\"</span>, <span class=\"pl-s1\">url</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"dbtable\"</span>, <span class=\"pl-s1\">table_name</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"user\"</span>, <span class=\"pl-s1\">username</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"password\"</span>, <span class=\"pl-s1\">password</span>).<span class=\"pl-en\">load</span>()</pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Azure Active Directory Authentication</h3><a id=\"user-content-azure-active-directory-authentication\" class=\"anchor\" aria-label=\"Permalink: Azure Active Directory Authentication\" href=\"#azure-active-directory-authentication\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h4 class=\"heading-element\" dir=\"auto\">Python Example with Service Principal</h4><a id=\"user-content-python-example-with-service-principal\" class=\"anchor\" aria-label=\"Permalink: Python Example with Service Principal\" href=\"#python-example-with-service-principal\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"context = adal.AuthenticationContext(authority)\ntoken = context.acquire_token_with_client_credentials(resource_app_id_url, service_principal_id, service_principal_secret)\naccess_token = token[&quot;accessToken&quot;]\n\njdbc_db = spark.read \\\n        .format(&quot;com.microsoft.sqlserver.jdbc.spark&quot;) \\\n        .option(&quot;url&quot;, url) \\\n        .option(&quot;dbtable&quot;, table_name) \\\n        .option(&quot;accessToken&quot;, access_token) \\\n        .option(&quot;encrypt&quot;, &quot;true&quot;) \\\n        .option(&quot;hostNameInCertificate&quot;, &quot;*.database.windows.net&quot;) \\\n        .load()\"><pre><span class=\"pl-s1\">context</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">adal</span>.<span class=\"pl-v\">AuthenticationContext</span>(<span class=\"pl-s1\">authority</span>)\n<span class=\"pl-s1\">token</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">context</span>.<span class=\"pl-en\">acquire_token_with_client_credentials</span>(<span class=\"pl-s1\">resource_app_id_url</span>, <span class=\"pl-s1\">service_principal_id</span>, <span class=\"pl-s1\">service_principal_secret</span>)\n<span class=\"pl-s1\">access_token</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">token</span>[<span class=\"pl-s\">\"accessToken\"</span>]\n\n<span class=\"pl-s1\">jdbc_db</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">spark</span>.<span class=\"pl-s1\">read</span> \\\n        .<span class=\"pl-en\">format</span>(<span class=\"pl-s\">\"com.microsoft.sqlserver.jdbc.spark\"</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"url\"</span>, <span class=\"pl-s1\">url</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"dbtable\"</span>, <span class=\"pl-s1\">table_name</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"accessToken\"</span>, <span class=\"pl-s1\">access_token</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"encrypt\"</span>, <span class=\"pl-s\">\"true\"</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"hostNameInCertificate\"</span>, <span class=\"pl-s\">\"*.database.windows.net\"</span>) \\\n        .<span class=\"pl-en\">load</span>()</pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h4 class=\"heading-element\" dir=\"auto\">Python Example with Active Directory Password</h4><a id=\"user-content-python-example-with-active-directory-password\" class=\"anchor\" aria-label=\"Permalink: Python Example with Active Directory Password\" href=\"#python-example-with-active-directory-password\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"jdbc_df = spark.read \\\n        .format(&quot;com.microsoft.sqlserver.jdbc.spark&quot;) \\\n        .option(&quot;url&quot;, url) \\\n        .option(&quot;dbtable&quot;, table_name) \\\n        .option(&quot;authentication&quot;, &quot;ActiveDirectoryPassword&quot;) \\\n        .option(&quot;user&quot;, user_name) \\\n        .option(&quot;password&quot;, password) \\\n        .option(&quot;encrypt&quot;, &quot;true&quot;) \\\n        .option(&quot;hostNameInCertificate&quot;, &quot;*.database.windows.net&quot;) \\\n        .load()\"><pre><span class=\"pl-s1\">jdbc_df</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">spark</span>.<span class=\"pl-s1\">read</span> \\\n        .<span class=\"pl-en\">format</span>(<span class=\"pl-s\">\"com.microsoft.sqlserver.jdbc.spark\"</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"url\"</span>, <span class=\"pl-s1\">url</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"dbtable\"</span>, <span class=\"pl-s1\">table_name</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"authentication\"</span>, <span class=\"pl-s\">\"ActiveDirectoryPassword\"</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"user\"</span>, <span class=\"pl-s1\">user_name</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"password\"</span>, <span class=\"pl-s1\">password</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"encrypt\"</span>, <span class=\"pl-s\">\"true\"</span>) \\\n        .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"hostNameInCertificate\"</span>, <span class=\"pl-s\">\"*.database.windows.net\"</span>) \\\n        .<span class=\"pl-en\">load</span>()</pre></div>\n<p dir=\"auto\">A required dependency must be installed in order to authenticate using\nActive Directory.</p>\n<p dir=\"auto\">For <strong>Scala,</strong> the <code>com.microsoft.aad.adal4j</code> artifact will need to be installed.</p>\n<p dir=\"auto\">For <strong>Python,</strong> the <code>adal</code> library will need to be installed.  This is available\nvia pip.</p>\n<p dir=\"auto\">Please check the <a href=\"samples\">sample notebooks</a> for examples.</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h1 class=\"heading-element\" dir=\"auto\">Support</h1><a id=\"user-content-support\" class=\"anchor\" aria-label=\"Permalink: Support\" href=\"#support\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">The Apache Spark Connector for Azure SQL and SQL Server is an open source project. This connector does not come with any Microsoft support. For issues with or questions about the connector, please create an Issue in this project repository. The connector community is active and monitoring submissions.</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h1 class=\"heading-element\" dir=\"auto\">Roadmap</h1><a id=\"user-content-roadmap\" class=\"anchor\" aria-label=\"Permalink: Roadmap\" href=\"#roadmap\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">Visit the Connector project in the <strong>Projects</strong> tab to see needed / planned items. Feel free to make an issue and start contributing!</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h1 class=\"heading-element\" dir=\"auto\">Contributing</h1><a id=\"user-content-contributing\" class=\"anchor\" aria-label=\"Permalink: Contributing\" href=\"#contributing\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">This project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit <a href=\"https://cla.opensource.microsoft.com\" rel=\"nofollow\">https://cla.opensource.microsoft.com</a>.</p>\n<p dir=\"auto\">When you submit a pull request, a CLA bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.</p>\n<p dir=\"auto\">This project has adopted the <a href=\"https://opensource.microsoft.com/codeofconduct/\" rel=\"nofollow\">Microsoft Open Source Code of Conduct</a>.\nFor more information see the <a href=\"https://opensource.microsoft.com/codeofconduct/faq/\" rel=\"nofollow\">Code of Conduct FAQ</a> or\ncontact <a href=\"mailto:opencode@microsoft.com\">opencode@microsoft.com</a> with any additional questions or comments.</p>\n</article></div>",
    "contributors" : [
      {
        "login" : "luxu1-ms",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/68044595?v=4",
        "url" : "https://github.com/luxu1-ms",
        "contributions" : 12
      },
      {
        "login" : "shivsood",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/1579057?v=4",
        "url" : "https://github.com/shivsood",
        "contributions" : 9
      },
      {
        "login" : "DaniBunny",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/743743?v=4",
        "url" : "https://github.com/DaniBunny",
        "contributions" : 5
      },
      {
        "login" : "arvindshmicrosoft",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/16342666?v=4",
        "url" : "https://github.com/arvindshmicrosoft",
        "contributions" : 3
      },
      {
        "login" : "cchighman",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/659214?v=4",
        "url" : "https://github.com/cchighman",
        "contributions" : 3
      },
      {
        "login" : "mokabiru",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/39162740?v=4",
        "url" : "https://github.com/mokabiru",
        "contributions" : 2
      },
      {
        "login" : "alexott",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/30342?v=4",
        "url" : "https://github.com/alexott",
        "contributions" : 1
      },
      {
        "login" : "moredatapls",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/11437312?v=4",
        "url" : "https://github.com/moredatapls",
        "contributions" : 1
      },
      {
        "login" : "microsoftopensource",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/22527892?v=4",
        "url" : "https://github.com/microsoftopensource",
        "contributions" : 1
      },
      {
        "login" : "denzilribeiro",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/12561620?v=4",
        "url" : "https://github.com/denzilribeiro",
        "contributions" : 1
      },
      {
        "login" : "microsoft-github-operations[bot]",
        "avatarUrl" : "https://avatars.githubusercontent.com/in/41902?v=4",
        "url" : "https://github.com/apps/microsoft-github-operations",
        "contributions" : 1
      },
      {
        "login" : "pp-akursar",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/123969479?v=4",
        "url" : "https://github.com/pp-akursar",
        "contributions" : 1
      }
    ],
    "commits" : 40,
    "topics" : [
    ],
    "contributingGuide" : null,
    "codeOfConduct" : null,
    "openIssues" : [
      {
        "number" : 266,
        "title" : "Issue while upgrading Spark Verison from 3.3 to 3.4 in Azure Synapse Studio.",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/266"
      },
      {
        "number" : 265,
        "title" : "New versions? Spark 3.3 and 3.4 - BETA?",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/265"
      },
      {
        "number" : 263,
        "title" : "append with bulk insert does results in NoSuchMethodError with DB runtime 12.2 LTS while overwrite works",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/263"
      },
      {
        "number" : 261,
        "title" : "Unable to delete rows",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/261"
      },
      {
        "number" : 260,
        "title" : "\"java.io.InvalidClassException: failed to read class descriptor\" Error when trying to write dataframe",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/260"
      },
      {
        "number" : 259,
        "title" : "Error when writing large dataframe to SQL Server",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/259"
      },
      {
        "number" : 258,
        "title" : "Spark 3.4 connector published on Maven central is misleading",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/258"
      },
      {
        "number" : 257,
        "title" : "False infos displayed: ",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/257"
      },
      {
        "number" : 256,
        "title" : "Support for Apache  Spark 3.4.x and 3.5.x",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/256"
      },
      {
        "number" : 255,
        "title" : "The project is dead or archived?",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/255"
      },
      {
        "number" : 254,
        "title" : "Intermittent Authentication Failure using ActiveDirectoryPassword",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/254"
      },
      {
        "number" : 253,
        "title" : "sql-spark-connector issue where the access token expires post 60 mins",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/253"
      },
      {
        "number" : 252,
        "title" : "Assessing the risk of duplicated entries for BEST_EFFORT reliabilityLevel",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/252"
      },
      {
        "number" : 251,
        "title" : "Support for Spark 3.4.x",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/251"
      },
      {
        "number" : 250,
        "title" : "Does bulk upsert data import support?",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/250"
      },
      {
        "number" : 249,
        "title" : "[Request] Support Spark 3.5",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/249"
      },
      {
        "number" : 248,
        "title" : "Version 8.4.1 of Microsoft JDBC Driver For SQL Server",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/248"
      },
      {
        "number" : 247,
        "title" : "Connection closed when try to use om.microsoft.sqlserver.jdbc.spark connector",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/247"
      },
      {
        "number" : 246,
        "title" : "TIMESTAMP to datetime2 fails on Azure Synapse",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/246"
      },
      {
        "number" : 245,
        "title" : "GA versions",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/245"
      },
      {
        "number" : 242,
        "title" : "Need the ability to use a linked service in a notebook to do a connection to a database",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/242"
      },
      {
        "number" : 240,
        "title" : "Missing Maven distros",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/240"
      },
      {
        "number" : 237,
        "title" : "[Question] how to set MAXDOP",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/237"
      },
      {
        "number" : 236,
        "title" : "writing with mode \"append\" to an existing table only rolls back faulty rows w/o \"NO_DUPLICATES\"",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/236"
      },
      {
        "number" : 234,
        "title" : "java.lang.ClassNotFoundException: com.microsoft.sqlserver.jdbc.SQLServerDriver",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/234"
      },
      {
        "number" : 233,
        "title" : " .format(\"com.microsoft.sqlserver.jdbc.spark\") does not work on Azure Databricks Spark 3.2+",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/233"
      },
      {
        "number" : 232,
        "title" : "Py4JJavaError: An error occurred while calling o866.load.",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/232"
      },
      {
        "number" : 231,
        "title" : "differing column nullable configurations when writing data that was immediately read ",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/231"
      },
      {
        "number" : 230,
        "title" : "Error with reliabilityLevel = NO_DUPLICATES and sequence in primary key",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/230"
      },
      {
        "number" : 229,
        "title" : "User class threw exception: java.lang.NoSuchMethodError: org.apache.spark.sql.jdbc.JdbcDialect.createConnectionFactory",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/229"
      },
      {
        "number" : 228,
        "title" : "Saving a column type of tinyint results in an error",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/228"
      },
      {
        "number" : 226,
        "title" : "How to pass column mappings?",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/226"
      },
      {
        "number" : 225,
        "title" : "Need support for scala version 2.13",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/225"
      },
      {
        "number" : 218,
        "title" : "not supported for java spark 3.3 scala 1.12?",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/218"
      },
      {
        "number" : 215,
        "title" : "Connection Error: Only when using AD Password Mode",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/215"
      },
      {
        "number" : 212,
        "title" : "Document support for Azure SQL Managed Instance",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/212"
      },
      {
        "number" : 207,
        "title" : "Writing from long column (nvarchar max) is very slow.",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/207"
      },
      {
        "number" : 201,
        "title" : "SQL server type conversion error during bulkCopy ",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/201"
      },
      {
        "number" : 200,
        "title" : "Provide OPTIONS in read path",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/200"
      },
      {
        "number" : 181,
        "title" : "MSAL Support",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/181"
      },
      {
        "number" : 179,
        "title" : "no internal transaction support for bulk insert?",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/179"
      },
      {
        "number" : 162,
        "title" : "Identity columns still causing \"Spark Dataframe and SQL Server table have differing numbers of columns\"",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/162"
      },
      {
        "number" : 123,
        "title" : "Dataframe.write with table containing Always generate columns and auto generate columns is failing ",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/123"
      },
      {
        "number" : 78,
        "title" : "Azure SQL Graph Architecture Support",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/78"
      },
      {
        "number" : 53,
        "title" : "Scala samples",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/53"
      },
      {
        "number" : 46,
        "title" : "Add support for load into staging table and that doing switch-in into partitioned table",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/46"
      },
      {
        "number" : 45,
        "title" : "Add support for ORDER hint ",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/45"
      },
      {
        "number" : 16,
        "title" : "DataSource V2 Implementation",
        "url" : "https://github.com/microsoft/sql-spark-connector/issues/16"
      }
    ],
    "scalaPercentage" : 95,
    "license" : "Apache-2.0",
    "commitActivity" : [
    ]
  },
  "settings" : {
    "preferStableVersion" : true,
    "defaultArtifact" : null,
    "customScalaDoc" : null,
    "documentationLinks" : [
    ],
    "contributorsWanted" : false,
    "deprecatedArtifacts" : [
    ],
    "cliArtifacts" : [
    ],
    "category" : "data-sources-and-connectors",
    "chatroom" : null
  }
}