{
  "organization" : "databricks",
  "repository" : "spark-csv",
  "creationDate" : 1419736446000,
  "githubStatus" : {
    "Ok" : {
      "updateDate" : 1730798461438
    }
  },
  "githubInfo" : {
    "homepage" : "http://databricks.com/",
    "description" : "CSV Data Source for Apache Spark 1.x",
    "logo" : "https://avatars.githubusercontent.com/u/4998052?v=4",
    "stars" : 1053,
    "forks" : 443,
    "watchers" : 428,
    "issues" : 204,
    "creationDate" : 1417568181000,
    "readme" : "<div id=\"readme\" class=\"md\" data-path=\"README.md\"><article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><div class=\"markdown-heading\" dir=\"auto\"><h1 class=\"heading-element\" dir=\"auto\">CSV Data Source for Apache Spark 1.x</h1><a id=\"user-content-csv-data-source-for-apache-spark-1x\" class=\"anchor\" aria-label=\"Permalink: CSV Data Source for Apache Spark 1.x\" href=\"#csv-data-source-for-apache-spark-1x\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\"><strong>NOTE: This functionality has been inlined in Apache Spark 2.x. This package is in maintenance mode and we only accept critical bug fixes.</strong></p>\n<p dir=\"auto\">A library for parsing and querying CSV data with Apache Spark, for Spark SQL and DataFrames.</p>\n<p dir=\"auto\"><a href=\"https://travis-ci.org/databricks/spark-csv\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/d3c62d798aa7aaaf9706e0d18d50912f0110bcf4f2b5a26440adef9c35d9944a/68747470733a2f2f7472617669732d63692e6f72672f64617461627269636b732f737061726b2d6373762e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/databricks/spark-csv.svg?branch=master\" style=\"max-width: 100%;\"></a>\n<a href=\"http://codecov.io/github/databricks/spark-csv?branch=master\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/633b8c3c23fdb35c6e7f8d0f2e7fafd63c093e8674742955e80419114ee092bd/687474703a2f2f636f6465636f762e696f2f6769746875622f64617461627269636b732f737061726b2d6373762f636f7665726167652e7376673f6272616e63683d6d6173746572\" alt=\"codecov.io\" data-canonical-src=\"http://codecov.io/github/databricks/spark-csv/coverage.svg?branch=master\" style=\"max-width: 100%;\"></a></p>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Requirements</h2><a id=\"user-content-requirements\" class=\"anchor\" aria-label=\"Permalink: Requirements\" href=\"#requirements\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">This library requires Spark 1.3+</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Linking</h2><a id=\"user-content-linking\" class=\"anchor\" aria-label=\"Permalink: Linking\" href=\"#linking\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">You can link against this library in your program at the following coordinates:</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Scala 2.10</h3><a id=\"user-content-scala-210\" class=\"anchor\" aria-label=\"Permalink: Scala 2.10\" href=\"#scala-210\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"groupId: com.databricks\nartifactId: spark-csv_2.10\nversion: 1.5.0\"><pre class=\"notranslate\"><code>groupId: com.databricks\nartifactId: spark-csv_2.10\nversion: 1.5.0\n</code></pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Scala 2.11</h3><a id=\"user-content-scala-211\" class=\"anchor\" aria-label=\"Permalink: Scala 2.11\" href=\"#scala-211\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"groupId: com.databricks\nartifactId: spark-csv_2.11\nversion: 1.5.0\"><pre class=\"notranslate\"><code>groupId: com.databricks\nartifactId: spark-csv_2.11\nversion: 1.5.0\n</code></pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Using with Spark shell</h2><a id=\"user-content-using-with-spark-shell\" class=\"anchor\" aria-label=\"Permalink: Using with Spark shell\" href=\"#using-with-spark-shell\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">This package can be added to  Spark using the <code>--packages</code> command line option.  For example, to include it when starting the spark shell:</p>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Spark compiled with Scala 2.11</h3><a id=\"user-content-spark-compiled-with-scala-211\" class=\"anchor\" aria-label=\"Permalink: Spark compiled with Scala 2.11\" href=\"#spark-compiled-with-scala-211\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.11:1.5.0\"><pre class=\"notranslate\"><code>$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.11:1.5.0\n</code></pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Spark compiled with Scala 2.10</h3><a id=\"user-content-spark-compiled-with-scala-210\" class=\"anchor\" aria-label=\"Permalink: Spark compiled with Scala 2.10\" href=\"#spark-compiled-with-scala-210\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.10:1.5.0\"><pre class=\"notranslate\"><code>$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.10:1.5.0\n</code></pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Features</h2><a id=\"user-content-features\" class=\"anchor\" aria-label=\"Permalink: Features\" href=\"#features\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">This package allows reading CSV files in local or distributed filesystem as <a href=\"https://spark.apache.org/docs/1.6.0/sql-programming-guide.html\" rel=\"nofollow\">Spark DataFrames</a>.\nWhen reading files the API accepts several options:</p>\n<ul dir=\"auto\">\n<li><code>path</code>: location of files. Similar to Spark can accept standard Hadoop globbing expressions.</li>\n<li><code>header</code>: when set to true the first line of files will be used to name columns and will not be included in data. All types will be assumed string. Default value is false.</li>\n<li><code>delimiter</code>: by default columns are delimited using <code>,</code>, but delimiter can be set to any character</li>\n<li><code>quote</code>: by default the quote character is <code>\"</code>, but can be set to any character. Delimiters inside quotes are ignored</li>\n<li><code>escape</code>: by default the escape character is <code>\\</code>, but can be set to any character. Escaped quote characters are ignored</li>\n<li><code>parserLib</code>: by default it is \"commons\" can be set to \"univocity\" to use that library for CSV parsing.</li>\n<li><code>mode</code>: determines the parsing mode. By default it is PERMISSIVE. Possible values are:\n<ul dir=\"auto\">\n<li><code>PERMISSIVE</code>: tries to parse all lines: nulls are inserted for missing tokens and extra tokens are ignored.</li>\n<li><code>DROPMALFORMED</code>: drops lines which have fewer or more tokens than expected or tokens which do\nnot match the schema</li>\n<li><code>FAILFAST</code>: aborts with a RuntimeException if encounters any malformed line</li>\n</ul>\n</li>\n<li><code>charset</code>: defaults to 'UTF-8' but can be set to other valid charset names</li>\n<li><code>inferSchema</code>: automatically infers column types. It requires one extra pass over the data and is false by default</li>\n<li><code>comment</code>: skip lines beginning with this character. Default is <code>\"#\"</code>. Disable comments by setting this to <code>null</code>.</li>\n<li><code>nullValue</code>: specifies a string that indicates a null value, any fields matching this string will be set as nulls in the DataFrame</li>\n<li><code>dateFormat</code>: specifies a string that indicates the date format to use when reading dates or timestamps. Custom date formats follow the formats at <a href=\"https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html\" rel=\"nofollow\"><code>java.text.SimpleDateFormat</code></a>. This applies to both <code>DateType</code> and <code>TimestampType</code>. By default, it is <code>null</code> which means trying to parse times and date by <code>java.sql.Timestamp.valueOf()</code> and <code>java.sql.Date.valueOf()</code>.</li>\n</ul>\n<p dir=\"auto\">The package also supports saving simple (non-nested) DataFrame. When writing files the API accepts several options:</p>\n<ul dir=\"auto\">\n<li><code>path</code>: location of files.</li>\n<li><code>header</code>: when set to true, the header (from the schema in the DataFrame) will be written at the first line.</li>\n<li><code>delimiter</code>: by default columns are delimited using <code>,</code>, but delimiter can be set to any character</li>\n<li><code>quote</code>: by default the quote character is <code>\"</code>, but can be set to any character. This is written according to <code>quoteMode</code>.</li>\n<li><code>escape</code>: by default the escape character is <code>\\</code>, but can be set to any character. Escaped quote characters are written.</li>\n<li><code>nullValue</code>: specifies a string that indicates a null value, nulls in the DataFrame will be written as this string.</li>\n<li><code>dateFormat</code>: specifies a string that indicates the date format to use writing dates or timestamps. Custom date formats follow the formats at <a href=\"https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html\" rel=\"nofollow\"><code>java.text.SimpleDateFormat</code></a>. This applies to both <code>DateType</code> and <code>TimestampType</code>. If no dateFormat is specified, then \"yyyy-MM-dd HH:mm:ss.S\".</li>\n<li><code>codec</code>: compression codec to use when saving to file. Should be the fully qualified name of a class implementing <code>org.apache.hadoop.io.compress.CompressionCodec</code> or one of case-insensitive shorten names (<code>bzip2</code>, <code>gzip</code>, <code>lz4</code>, and <code>snappy</code>). Defaults to no compression when a codec is not specified.</li>\n<li><code>quoteMode</code>: when to quote fields (<code>ALL</code>, <code>MINIMAL</code> (default), <code>NON_NUMERIC</code>, <code>NONE</code>), see <a href=\"https://commons.apache.org/proper/commons-csv/apidocs/org/apache/commons/csv/QuoteMode.html\" rel=\"nofollow\">Quote Modes</a></li>\n</ul>\n<p dir=\"auto\">These examples use a CSV file available for download <a href=\"https://github.com/databricks/spark-csv/raw/master/src/test/resources/cars.csv\">here</a>:</p>\n<div class=\"snippet-clipboard-content notranslate position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"$ wget https://github.com/databricks/spark-csv/raw/master/src/test/resources/cars.csv\"><pre class=\"notranslate\"><code>$ wget https://github.com/databricks/spark-csv/raw/master/src/test/resources/cars.csv\n</code></pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">SQL API</h3><a id=\"user-content-sql-api\" class=\"anchor\" aria-label=\"Permalink: SQL API\" href=\"#sql-api\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">CSV data source for Spark can infer data types:</p>\n<div class=\"highlight highlight-source-sql notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"CREATE TABLE cars\nUSING com.databricks.spark.csv\nOPTIONS (path &quot;cars.csv&quot;, header &quot;true&quot;, inferSchema &quot;true&quot;)\"><pre><span class=\"pl-k\">CREATE</span> <span class=\"pl-k\">TABLE</span> <span class=\"pl-en\">cars</span>\nUSING <span class=\"pl-c1\">com</span>.<span class=\"pl-c1\">databricks</span>.<span class=\"pl-c1\">spark</span>.<span class=\"pl-c1\">csv</span>\nOPTIONS (<span class=\"pl-k\">path</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cars.csv<span class=\"pl-pds\">\"</span></span>, header <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>, inferSchema <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p dir=\"auto\">You can also specify column names and types in DDL.</p>\n<div class=\"highlight highlight-source-sql notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"CREATE TABLE cars (yearMade double, carMake string, carModel string, comments string, blank string)\nUSING com.databricks.spark.csv\nOPTIONS (path &quot;cars.csv&quot;, header &quot;true&quot;)\"><pre><span class=\"pl-k\">CREATE</span> <span class=\"pl-k\">TABLE</span> <span class=\"pl-en\">cars</span> (yearMade double, carMake string, carModel string, comments string, blank string)\nUSING <span class=\"pl-c1\">com</span>.<span class=\"pl-c1\">databricks</span>.<span class=\"pl-c1\">spark</span>.<span class=\"pl-c1\">csv</span>\nOPTIONS (<span class=\"pl-k\">path</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cars.csv<span class=\"pl-pds\">\"</span></span>, header <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Scala API</h3><a id=\"user-content-scala-api\" class=\"anchor\" aria-label=\"Permalink: Scala API\" href=\"#scala-api\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\"><strong>Spark 1.4+:</strong></p>\n<p dir=\"auto\">Automatically infer schema (data types), otherwise everything is assumed string:</p>\n<div class=\"highlight highlight-source-scala notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import org.apache.spark.sql.SQLContext\n\nval sqlContext = new SQLContext(sc)\nval df = sqlContext.read\n    .format(&quot;com.databricks.spark.csv&quot;)\n    .option(&quot;header&quot;, &quot;true&quot;) // Use first line of all files as header\n    .option(&quot;inferSchema&quot;, &quot;true&quot;) // Automatically infer data types\n    .load(&quot;cars.csv&quot;)\n\nval selectedData = df.select(&quot;year&quot;, &quot;model&quot;)\nselectedData.write\n    .format(&quot;com.databricks.spark.csv&quot;)\n    .option(&quot;header&quot;, &quot;true&quot;)\n    .save(&quot;newcars.csv&quot;)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-en\">org</span>.<span class=\"pl-en\">apache</span>.<span class=\"pl-en\">spark</span>.<span class=\"pl-en\">sql</span>.<span class=\"pl-en\">SQLContext</span>\n\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">sqlContext</span> <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-en\">SQLContext</span>(sc)\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">df</span> <span class=\"pl-k\">=</span> sqlContext.read\n    .format(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>)\n    .option(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>header<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-c\"><span class=\"pl-c\">//</span> Use first line of all files as header</span>\n    .option(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>inferSchema<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-c\"><span class=\"pl-c\">//</span> Automatically infer data types</span>\n    .load(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cars.csv<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">selectedData</span> <span class=\"pl-k\">=</span> df.select(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>year<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>model<span class=\"pl-pds\">\"</span></span>)\nselectedData.write\n    .format(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>)\n    .option(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>header<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>)\n    .save(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>newcars.csv<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p dir=\"auto\">You can manually specify the schema when reading data:</p>\n<div class=\"highlight highlight-source-scala notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType}\n\nval sqlContext = new SQLContext(sc)\nval customSchema = StructType(Array(\n    StructField(&quot;year&quot;, IntegerType, true),\n    StructField(&quot;make&quot;, StringType, true),\n    StructField(&quot;model&quot;, StringType, true),\n    StructField(&quot;comment&quot;, StringType, true),\n    StructField(&quot;blank&quot;, StringType, true)))\n\nval df = sqlContext.read\n    .format(&quot;com.databricks.spark.csv&quot;)\n    .option(&quot;header&quot;, &quot;true&quot;) // Use first line of all files as header\n    .schema(customSchema)\n    .load(&quot;cars.csv&quot;)\n\nval selectedData = df.select(&quot;year&quot;, &quot;model&quot;)\nselectedData.write\n    .format(&quot;com.databricks.spark.csv&quot;)\n    .option(&quot;header&quot;, &quot;true&quot;)\n    .save(&quot;newcars.csv&quot;)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-en\">org</span>.<span class=\"pl-en\">apache</span>.<span class=\"pl-en\">spark</span>.<span class=\"pl-en\">sql</span>.<span class=\"pl-en\">SQLContext</span>\n<span class=\"pl-k\">import</span> <span class=\"pl-en\">org</span>.<span class=\"pl-en\">apache</span>.<span class=\"pl-en\">spark</span>.<span class=\"pl-en\">sql</span>.<span class=\"pl-en\">types</span>.{<span class=\"pl-en\">StructType</span>, <span class=\"pl-en\">StructField</span>, <span class=\"pl-en\">StringType</span>, <span class=\"pl-en\">IntegerType</span>}\n\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">sqlContext</span> <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-en\">SQLContext</span>(sc)\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">customSchema</span> <span class=\"pl-k\">=</span> <span class=\"pl-en\">StructType</span>(<span class=\"pl-en\">Array</span>(\n    <span class=\"pl-en\">StructField</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>year<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-en\">IntegerType</span>, <span class=\"pl-c1\">true</span>),\n    <span class=\"pl-en\">StructField</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>make<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-en\">StringType</span>, <span class=\"pl-c1\">true</span>),\n    <span class=\"pl-en\">StructField</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>model<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-en\">StringType</span>, <span class=\"pl-c1\">true</span>),\n    <span class=\"pl-en\">StructField</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>comment<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-en\">StringType</span>, <span class=\"pl-c1\">true</span>),\n    <span class=\"pl-en\">StructField</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>blank<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-en\">StringType</span>, <span class=\"pl-c1\">true</span>)))\n\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">df</span> <span class=\"pl-k\">=</span> sqlContext.read\n    .format(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>)\n    .option(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>header<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-c\"><span class=\"pl-c\">//</span> Use first line of all files as header</span>\n    .schema(customSchema)\n    .load(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cars.csv<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">selectedData</span> <span class=\"pl-k\">=</span> df.select(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>year<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>model<span class=\"pl-pds\">\"</span></span>)\nselectedData.write\n    .format(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>)\n    .option(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>header<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>)\n    .save(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>newcars.csv<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p dir=\"auto\">You can save with compressed output:</p>\n<div class=\"highlight highlight-source-scala notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import org.apache.spark.sql.SQLContext\n\nval sqlContext = new SQLContext(sc)\nval df = sqlContext.read\n    .format(&quot;com.databricks.spark.csv&quot;)\n    .option(&quot;header&quot;, &quot;true&quot;) // Use first line of all files as header\n    .option(&quot;inferSchema&quot;, &quot;true&quot;) // Automatically infer data types\n    .load(&quot;cars.csv&quot;)\n\nval selectedData = df.select(&quot;year&quot;, &quot;model&quot;)\nselectedData.write\n    .format(&quot;com.databricks.spark.csv&quot;)\n    .option(&quot;header&quot;, &quot;true&quot;)\n    .option(&quot;codec&quot;, &quot;org.apache.hadoop.io.compress.GzipCodec&quot;)\n    .save(&quot;newcars.csv.gz&quot;)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-en\">org</span>.<span class=\"pl-en\">apache</span>.<span class=\"pl-en\">spark</span>.<span class=\"pl-en\">sql</span>.<span class=\"pl-en\">SQLContext</span>\n\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">sqlContext</span> <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-en\">SQLContext</span>(sc)\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">df</span> <span class=\"pl-k\">=</span> sqlContext.read\n    .format(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>)\n    .option(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>header<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-c\"><span class=\"pl-c\">//</span> Use first line of all files as header</span>\n    .option(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>inferSchema<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-c\"><span class=\"pl-c\">//</span> Automatically infer data types</span>\n    .load(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cars.csv<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">selectedData</span> <span class=\"pl-k\">=</span> df.select(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>year<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>model<span class=\"pl-pds\">\"</span></span>)\nselectedData.write\n    .format(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>)\n    .option(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>header<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>)\n    .option(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>codec<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>org.apache.hadoop.io.compress.GzipCodec<span class=\"pl-pds\">\"</span></span>)\n    .save(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>newcars.csv.gz<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p dir=\"auto\"><strong>Spark 1.3:</strong></p>\n<p dir=\"auto\">Automatically infer schema (data types), otherwise everything is assumed string:</p>\n<div class=\"highlight highlight-source-scala notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import org.apache.spark.sql.SQLContext\n\nval sqlContext = new SQLContext(sc)\nval df = sqlContext.load(\n    &quot;com.databricks.spark.csv&quot;,\n    Map(&quot;path&quot; -&gt; &quot;cars.csv&quot;, &quot;header&quot; -&gt; &quot;true&quot;, &quot;inferSchema&quot; -&gt; &quot;true&quot;))\nval selectedData = df.select(&quot;year&quot;, &quot;model&quot;)\nselectedData.save(&quot;newcars.csv&quot;, &quot;com.databricks.spark.csv&quot;)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-en\">org</span>.<span class=\"pl-en\">apache</span>.<span class=\"pl-en\">spark</span>.<span class=\"pl-en\">sql</span>.<span class=\"pl-en\">SQLContext</span>\n\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">sqlContext</span> <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-en\">SQLContext</span>(sc)\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">df</span> <span class=\"pl-k\">=</span> sqlContext.load(\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>,\n    <span class=\"pl-en\">Map</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>path<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">-</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cars.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>header<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">-</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>inferSchema<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">-</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>))\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">selectedData</span> <span class=\"pl-k\">=</span> df.select(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>year<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>model<span class=\"pl-pds\">\"</span></span>)\nselectedData.save(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>newcars.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p dir=\"auto\">You can manually specify the schema when reading data:</p>\n<div class=\"highlight highlight-source-scala notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import org.apache.spark.sql.SQLContext\nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType};\n\nval sqlContext = new SQLContext(sc)\nval customSchema = StructType(Array(\n    StructField(&quot;year&quot;, IntegerType, true),\n    StructField(&quot;make&quot;, StringType, true),\n    StructField(&quot;model&quot;, StringType, true),\n    StructField(&quot;comment&quot;, StringType, true),\n    StructField(&quot;blank&quot;, StringType, true)))\n\nval df = sqlContext.load(\n    &quot;com.databricks.spark.csv&quot;,\n    schema = customSchema,\n    Map(&quot;path&quot; -&gt; &quot;cars.csv&quot;, &quot;header&quot; -&gt; &quot;true&quot;))\n\nval selectedData = df.select(&quot;year&quot;, &quot;model&quot;)\nselectedData.save(&quot;newcars.csv&quot;, &quot;com.databricks.spark.csv&quot;)\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-en\">org</span>.<span class=\"pl-en\">apache</span>.<span class=\"pl-en\">spark</span>.<span class=\"pl-en\">sql</span>.<span class=\"pl-en\">SQLContext</span>\n<span class=\"pl-k\">import</span> <span class=\"pl-en\">org</span>.<span class=\"pl-en\">apache</span>.<span class=\"pl-en\">spark</span>.<span class=\"pl-en\">sql</span>.<span class=\"pl-en\">types</span>.{<span class=\"pl-en\">StructType</span>, <span class=\"pl-en\">StructField</span>, <span class=\"pl-en\">StringType</span>, <span class=\"pl-en\">IntegerType</span>};\n\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">sqlContext</span> <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-en\">SQLContext</span>(sc)\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">customSchema</span> <span class=\"pl-k\">=</span> <span class=\"pl-en\">StructType</span>(<span class=\"pl-en\">Array</span>(\n    <span class=\"pl-en\">StructField</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>year<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-en\">IntegerType</span>, <span class=\"pl-c1\">true</span>),\n    <span class=\"pl-en\">StructField</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>make<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-en\">StringType</span>, <span class=\"pl-c1\">true</span>),\n    <span class=\"pl-en\">StructField</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>model<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-en\">StringType</span>, <span class=\"pl-c1\">true</span>),\n    <span class=\"pl-en\">StructField</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>comment<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-en\">StringType</span>, <span class=\"pl-c1\">true</span>),\n    <span class=\"pl-en\">StructField</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>blank<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-en\">StringType</span>, <span class=\"pl-c1\">true</span>)))\n\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">df</span> <span class=\"pl-k\">=</span> sqlContext.load(\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>,\n    schema <span class=\"pl-k\">=</span> customSchema,\n    <span class=\"pl-en\">Map</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>path<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">-</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cars.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>header<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">-</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>))\n\n<span class=\"pl-k\">val</span> <span class=\"pl-v\">selectedData</span> <span class=\"pl-k\">=</span> df.select(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>year<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>model<span class=\"pl-pds\">\"</span></span>)\nselectedData.save(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>newcars.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Java API</h3><a id=\"user-content-java-api\" class=\"anchor\" aria-label=\"Permalink: Java API\" href=\"#java-api\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\"><strong>Spark 1.4+:</strong></p>\n<p dir=\"auto\">Automatically infer schema (data types), otherwise everything is assumed string:</p>\n<div class=\"highlight highlight-source-java notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import org.apache.spark.sql.SQLContext\n\nSQLContext sqlContext = new SQLContext(sc);\nDataFrame df = sqlContext.read()\n    .format(&quot;com.databricks.spark.csv&quot;)\n    .option(&quot;inferSchema&quot;, &quot;true&quot;)\n    .option(&quot;header&quot;, &quot;true&quot;)\n    .load(&quot;cars.csv&quot;);\n\ndf.select(&quot;year&quot;, &quot;model&quot;).write()\n    .format(&quot;com.databricks.spark.csv&quot;)\n    .option(&quot;header&quot;, &quot;true&quot;)\n    .save(&quot;newcars.csv&quot;);\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">org</span>.<span class=\"pl-s1\">apache</span>.<span class=\"pl-s1\">spark</span>.<span class=\"pl-s1\">sql</span>.<span class=\"pl-s1\">SQLContext</span>\n\n<span class=\"pl-smi\">SQLContext</span> <span class=\"pl-s1\">sqlContext</span> = <span class=\"pl-k\">new</span> <span class=\"pl-smi\">SQLContext</span>(<span class=\"pl-s1\">sc</span>);\n<span class=\"pl-smi\">DataFrame</span> <span class=\"pl-s1\">df</span> = <span class=\"pl-s1\">sqlContext</span>.<span class=\"pl-en\">read</span>()\n    .<span class=\"pl-en\">format</span>(<span class=\"pl-s\">\"com.databricks.spark.csv\"</span>)\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"inferSchema\"</span>, <span class=\"pl-s\">\"true\"</span>)\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"header\"</span>, <span class=\"pl-s\">\"true\"</span>)\n    .<span class=\"pl-en\">load</span>(<span class=\"pl-s\">\"cars.csv\"</span>);\n\n<span class=\"pl-s1\">df</span>.<span class=\"pl-en\">select</span>(<span class=\"pl-s\">\"year\"</span>, <span class=\"pl-s\">\"model\"</span>).<span class=\"pl-en\">write</span>()\n    .<span class=\"pl-en\">format</span>(<span class=\"pl-s\">\"com.databricks.spark.csv\"</span>)\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"header\"</span>, <span class=\"pl-s\">\"true\"</span>)\n    .<span class=\"pl-en\">save</span>(<span class=\"pl-s\">\"newcars.csv\"</span>);</pre></div>\n<p dir=\"auto\">You can manually specify schema:</p>\n<div class=\"highlight highlight-source-java notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import org.apache.spark.sql.SQLContext;\nimport org.apache.spark.sql.types.*;\n\nSQLContext sqlContext = new SQLContext(sc);\nStructType customSchema = new StructType(new StructField[] {\n    new StructField(&quot;year&quot;, DataTypes.IntegerType, true, Metadata.empty()),\n    new StructField(&quot;make&quot;, DataTypes.StringType, true, Metadata.empty()),\n    new StructField(&quot;model&quot;, DataTypes.StringType, true, Metadata.empty()),\n    new StructField(&quot;comment&quot;, DataTypes.StringType, true, Metadata.empty()),\n    new StructField(&quot;blank&quot;, DataTypes.StringType, true, Metadata.empty())\n});\n\nDataFrame df = sqlContext.read()\n    .format(&quot;com.databricks.spark.csv&quot;)\n    .schema(customSchema)\n    .option(&quot;header&quot;, &quot;true&quot;)\n    .load(&quot;cars.csv&quot;);\n\ndf.select(&quot;year&quot;, &quot;model&quot;).write()\n    .format(&quot;com.databricks.spark.csv&quot;)\n    .option(&quot;header&quot;, &quot;true&quot;)\n    .save(&quot;newcars.csv&quot;);\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">org</span>.<span class=\"pl-s1\">apache</span>.<span class=\"pl-s1\">spark</span>.<span class=\"pl-s1\">sql</span>.<span class=\"pl-s1\">SQLContext</span>;\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">org</span>.<span class=\"pl-s1\">apache</span>.<span class=\"pl-s1\">spark</span>.<span class=\"pl-s1\">sql</span>.<span class=\"pl-s1\">types</span>.*;\n\n<span class=\"pl-smi\">SQLContext</span> <span class=\"pl-s1\">sqlContext</span> = <span class=\"pl-k\">new</span> <span class=\"pl-smi\">SQLContext</span>(<span class=\"pl-s1\">sc</span>);\n<span class=\"pl-smi\">StructType</span> <span class=\"pl-s1\">customSchema</span> = <span class=\"pl-k\">new</span> <span class=\"pl-smi\">StructType</span>(<span class=\"pl-k\">new</span> <span class=\"pl-smi\">StructField</span>[] {\n    <span class=\"pl-k\">new</span> <span class=\"pl-smi\">StructField</span>(<span class=\"pl-s\">\"year\"</span>, <span class=\"pl-smi\">DataTypes</span>.<span class=\"pl-s1\">IntegerType</span>, <span class=\"pl-c1\">true</span>, <span class=\"pl-smi\">Metadata</span>.<span class=\"pl-en\">empty</span>()),\n    <span class=\"pl-k\">new</span> <span class=\"pl-smi\">StructField</span>(<span class=\"pl-s\">\"make\"</span>, <span class=\"pl-smi\">DataTypes</span>.<span class=\"pl-s1\">StringType</span>, <span class=\"pl-c1\">true</span>, <span class=\"pl-smi\">Metadata</span>.<span class=\"pl-en\">empty</span>()),\n    <span class=\"pl-k\">new</span> <span class=\"pl-smi\">StructField</span>(<span class=\"pl-s\">\"model\"</span>, <span class=\"pl-smi\">DataTypes</span>.<span class=\"pl-s1\">StringType</span>, <span class=\"pl-c1\">true</span>, <span class=\"pl-smi\">Metadata</span>.<span class=\"pl-en\">empty</span>()),\n    <span class=\"pl-k\">new</span> <span class=\"pl-smi\">StructField</span>(<span class=\"pl-s\">\"comment\"</span>, <span class=\"pl-smi\">DataTypes</span>.<span class=\"pl-s1\">StringType</span>, <span class=\"pl-c1\">true</span>, <span class=\"pl-smi\">Metadata</span>.<span class=\"pl-en\">empty</span>()),\n    <span class=\"pl-k\">new</span> <span class=\"pl-smi\">StructField</span>(<span class=\"pl-s\">\"blank\"</span>, <span class=\"pl-smi\">DataTypes</span>.<span class=\"pl-s1\">StringType</span>, <span class=\"pl-c1\">true</span>, <span class=\"pl-smi\">Metadata</span>.<span class=\"pl-en\">empty</span>())\n});\n\n<span class=\"pl-smi\">DataFrame</span> <span class=\"pl-s1\">df</span> = <span class=\"pl-s1\">sqlContext</span>.<span class=\"pl-en\">read</span>()\n    .<span class=\"pl-en\">format</span>(<span class=\"pl-s\">\"com.databricks.spark.csv\"</span>)\n    .<span class=\"pl-en\">schema</span>(<span class=\"pl-s1\">customSchema</span>)\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"header\"</span>, <span class=\"pl-s\">\"true\"</span>)\n    .<span class=\"pl-en\">load</span>(<span class=\"pl-s\">\"cars.csv\"</span>);\n\n<span class=\"pl-s1\">df</span>.<span class=\"pl-en\">select</span>(<span class=\"pl-s\">\"year\"</span>, <span class=\"pl-s\">\"model\"</span>).<span class=\"pl-en\">write</span>()\n    .<span class=\"pl-en\">format</span>(<span class=\"pl-s\">\"com.databricks.spark.csv\"</span>)\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"header\"</span>, <span class=\"pl-s\">\"true\"</span>)\n    .<span class=\"pl-en\">save</span>(<span class=\"pl-s\">\"newcars.csv\"</span>);</pre></div>\n<p dir=\"auto\">You can save with compressed output:</p>\n<div class=\"highlight highlight-source-java notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import org.apache.spark.sql.SQLContext\n\nSQLContext sqlContext = new SQLContext(sc);\nDataFrame df = sqlContext.read()\n    .format(&quot;com.databricks.spark.csv&quot;)\n    .option(&quot;inferSchema&quot;, &quot;true&quot;)\n    .option(&quot;header&quot;, &quot;true&quot;)\n    .load(&quot;cars.csv&quot;);\n\ndf.select(&quot;year&quot;, &quot;model&quot;).write()\n    .format(&quot;com.databricks.spark.csv&quot;)\n    .option(&quot;header&quot;, &quot;true&quot;)\n    .option(&quot;codec&quot;, &quot;org.apache.hadoop.io.compress.GzipCodec&quot;)\n    .save(&quot;newcars.csv&quot;);\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">org</span>.<span class=\"pl-s1\">apache</span>.<span class=\"pl-s1\">spark</span>.<span class=\"pl-s1\">sql</span>.<span class=\"pl-s1\">SQLContext</span>\n\n<span class=\"pl-smi\">SQLContext</span> <span class=\"pl-s1\">sqlContext</span> = <span class=\"pl-k\">new</span> <span class=\"pl-smi\">SQLContext</span>(<span class=\"pl-s1\">sc</span>);\n<span class=\"pl-smi\">DataFrame</span> <span class=\"pl-s1\">df</span> = <span class=\"pl-s1\">sqlContext</span>.<span class=\"pl-en\">read</span>()\n    .<span class=\"pl-en\">format</span>(<span class=\"pl-s\">\"com.databricks.spark.csv\"</span>)\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"inferSchema\"</span>, <span class=\"pl-s\">\"true\"</span>)\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"header\"</span>, <span class=\"pl-s\">\"true\"</span>)\n    .<span class=\"pl-en\">load</span>(<span class=\"pl-s\">\"cars.csv\"</span>);\n\n<span class=\"pl-s1\">df</span>.<span class=\"pl-en\">select</span>(<span class=\"pl-s\">\"year\"</span>, <span class=\"pl-s\">\"model\"</span>).<span class=\"pl-en\">write</span>()\n    .<span class=\"pl-en\">format</span>(<span class=\"pl-s\">\"com.databricks.spark.csv\"</span>)\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"header\"</span>, <span class=\"pl-s\">\"true\"</span>)\n    .<span class=\"pl-en\">option</span>(<span class=\"pl-s\">\"codec\"</span>, <span class=\"pl-s\">\"org.apache.hadoop.io.compress.GzipCodec\"</span>)\n    .<span class=\"pl-en\">save</span>(<span class=\"pl-s\">\"newcars.csv\"</span>);</pre></div>\n<p dir=\"auto\"><strong>Spark 1.3:</strong></p>\n<p dir=\"auto\">Automatically infer schema (data types), otherwise everything is assumed string:</p>\n<div class=\"highlight highlight-source-java notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import org.apache.spark.sql.SQLContext\n\nSQLContext sqlContext = new SQLContext(sc);\n\nHashMap&lt;String, String&gt; options = new HashMap&lt;String, String&gt;();\noptions.put(&quot;header&quot;, &quot;true&quot;);\noptions.put(&quot;path&quot;, &quot;cars.csv&quot;);\noptions.put(&quot;inferSchema&quot;, &quot;true&quot;);\n\nDataFrame df = sqlContext.load(&quot;com.databricks.spark.csv&quot;, options);\ndf.select(&quot;year&quot;, &quot;model&quot;).save(&quot;newcars.csv&quot;, &quot;com.databricks.spark.csv&quot;);\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">org</span>.<span class=\"pl-s1\">apache</span>.<span class=\"pl-s1\">spark</span>.<span class=\"pl-s1\">sql</span>.<span class=\"pl-s1\">SQLContext</span>\n\n<span class=\"pl-smi\">SQLContext</span> <span class=\"pl-s1\">sqlContext</span> = <span class=\"pl-k\">new</span> <span class=\"pl-smi\">SQLContext</span>(<span class=\"pl-s1\">sc</span>);\n\n<span class=\"pl-smi\">HashMap</span>&lt;<span class=\"pl-smi\">String</span>, <span class=\"pl-smi\">String</span>&gt; <span class=\"pl-s1\">options</span> = <span class=\"pl-k\">new</span> <span class=\"pl-smi\">HashMap</span>&lt;<span class=\"pl-smi\">String</span>, <span class=\"pl-smi\">String</span>&gt;();\n<span class=\"pl-s1\">options</span>.<span class=\"pl-en\">put</span>(<span class=\"pl-s\">\"header\"</span>, <span class=\"pl-s\">\"true\"</span>);\n<span class=\"pl-s1\">options</span>.<span class=\"pl-en\">put</span>(<span class=\"pl-s\">\"path\"</span>, <span class=\"pl-s\">\"cars.csv\"</span>);\n<span class=\"pl-s1\">options</span>.<span class=\"pl-en\">put</span>(<span class=\"pl-s\">\"inferSchema\"</span>, <span class=\"pl-s\">\"true\"</span>);\n\n<span class=\"pl-smi\">DataFrame</span> <span class=\"pl-s1\">df</span> = <span class=\"pl-s1\">sqlContext</span>.<span class=\"pl-en\">load</span>(<span class=\"pl-s\">\"com.databricks.spark.csv\"</span>, <span class=\"pl-s1\">options</span>);\n<span class=\"pl-s1\">df</span>.<span class=\"pl-en\">select</span>(<span class=\"pl-s\">\"year\"</span>, <span class=\"pl-s\">\"model\"</span>).<span class=\"pl-en\">save</span>(<span class=\"pl-s\">\"newcars.csv\"</span>, <span class=\"pl-s\">\"com.databricks.spark.csv\"</span>);</pre></div>\n<p dir=\"auto\">You can manually specify schema:</p>\n<div class=\"highlight highlight-source-java notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import org.apache.spark.sql.SQLContext;\nimport org.apache.spark.sql.types.*;\n\nSQLContext sqlContext = new SQLContext(sc);\nStructType customSchema = new StructType(new StructField[] {\n    new StructField(&quot;year&quot;, DataTypes.IntegerType, true, Metadata.empty()),\n    new StructField(&quot;make&quot;, DataTypes.StringType, true, Metadata.empty()),\n    new StructField(&quot;model&quot;, DataTypes.StringType, true, Metadata.empty()),\n    new StructField(&quot;comment&quot;, DataTypes.StringType, true, Metadata.empty()),\n    new StructField(&quot;blank&quot;, DataTypes.StringType, true, Metadata.empty())\n});\n\nHashMap&lt;String, String&gt; options = new HashMap&lt;String, String&gt;();\noptions.put(&quot;header&quot;, &quot;true&quot;);\noptions.put(&quot;path&quot;, &quot;cars.csv&quot;);\n\nDataFrame df = sqlContext.load(&quot;com.databricks.spark.csv&quot;, customSchema, options);\ndf.select(&quot;year&quot;, &quot;model&quot;).save(&quot;newcars.csv&quot;, &quot;com.databricks.spark.csv&quot;);\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">org</span>.<span class=\"pl-s1\">apache</span>.<span class=\"pl-s1\">spark</span>.<span class=\"pl-s1\">sql</span>.<span class=\"pl-s1\">SQLContext</span>;\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">org</span>.<span class=\"pl-s1\">apache</span>.<span class=\"pl-s1\">spark</span>.<span class=\"pl-s1\">sql</span>.<span class=\"pl-s1\">types</span>.*;\n\n<span class=\"pl-smi\">SQLContext</span> <span class=\"pl-s1\">sqlContext</span> = <span class=\"pl-k\">new</span> <span class=\"pl-smi\">SQLContext</span>(<span class=\"pl-s1\">sc</span>);\n<span class=\"pl-smi\">StructType</span> <span class=\"pl-s1\">customSchema</span> = <span class=\"pl-k\">new</span> <span class=\"pl-smi\">StructType</span>(<span class=\"pl-k\">new</span> <span class=\"pl-smi\">StructField</span>[] {\n    <span class=\"pl-k\">new</span> <span class=\"pl-smi\">StructField</span>(<span class=\"pl-s\">\"year\"</span>, <span class=\"pl-smi\">DataTypes</span>.<span class=\"pl-s1\">IntegerType</span>, <span class=\"pl-c1\">true</span>, <span class=\"pl-smi\">Metadata</span>.<span class=\"pl-en\">empty</span>()),\n    <span class=\"pl-k\">new</span> <span class=\"pl-smi\">StructField</span>(<span class=\"pl-s\">\"make\"</span>, <span class=\"pl-smi\">DataTypes</span>.<span class=\"pl-s1\">StringType</span>, <span class=\"pl-c1\">true</span>, <span class=\"pl-smi\">Metadata</span>.<span class=\"pl-en\">empty</span>()),\n    <span class=\"pl-k\">new</span> <span class=\"pl-smi\">StructField</span>(<span class=\"pl-s\">\"model\"</span>, <span class=\"pl-smi\">DataTypes</span>.<span class=\"pl-s1\">StringType</span>, <span class=\"pl-c1\">true</span>, <span class=\"pl-smi\">Metadata</span>.<span class=\"pl-en\">empty</span>()),\n    <span class=\"pl-k\">new</span> <span class=\"pl-smi\">StructField</span>(<span class=\"pl-s\">\"comment\"</span>, <span class=\"pl-smi\">DataTypes</span>.<span class=\"pl-s1\">StringType</span>, <span class=\"pl-c1\">true</span>, <span class=\"pl-smi\">Metadata</span>.<span class=\"pl-en\">empty</span>()),\n    <span class=\"pl-k\">new</span> <span class=\"pl-smi\">StructField</span>(<span class=\"pl-s\">\"blank\"</span>, <span class=\"pl-smi\">DataTypes</span>.<span class=\"pl-s1\">StringType</span>, <span class=\"pl-c1\">true</span>, <span class=\"pl-smi\">Metadata</span>.<span class=\"pl-en\">empty</span>())\n});\n\n<span class=\"pl-smi\">HashMap</span>&lt;<span class=\"pl-smi\">String</span>, <span class=\"pl-smi\">String</span>&gt; <span class=\"pl-s1\">options</span> = <span class=\"pl-k\">new</span> <span class=\"pl-smi\">HashMap</span>&lt;<span class=\"pl-smi\">String</span>, <span class=\"pl-smi\">String</span>&gt;();\n<span class=\"pl-s1\">options</span>.<span class=\"pl-en\">put</span>(<span class=\"pl-s\">\"header\"</span>, <span class=\"pl-s\">\"true\"</span>);\n<span class=\"pl-s1\">options</span>.<span class=\"pl-en\">put</span>(<span class=\"pl-s\">\"path\"</span>, <span class=\"pl-s\">\"cars.csv\"</span>);\n\n<span class=\"pl-smi\">DataFrame</span> <span class=\"pl-s1\">df</span> = <span class=\"pl-s1\">sqlContext</span>.<span class=\"pl-en\">load</span>(<span class=\"pl-s\">\"com.databricks.spark.csv\"</span>, <span class=\"pl-s1\">customSchema</span>, <span class=\"pl-s1\">options</span>);\n<span class=\"pl-s1\">df</span>.<span class=\"pl-en\">select</span>(<span class=\"pl-s\">\"year\"</span>, <span class=\"pl-s\">\"model\"</span>).<span class=\"pl-en\">save</span>(<span class=\"pl-s\">\"newcars.csv\"</span>, <span class=\"pl-s\">\"com.databricks.spark.csv\"</span>);</pre></div>\n<p dir=\"auto\">You can save with compressed output:</p>\n<div class=\"highlight highlight-source-java notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"import org.apache.spark.sql.SQLContext;\nimport org.apache.spark.sql.SaveMode;\n\nSQLContext sqlContext = new SQLContext(sc);\n\nHashMap&lt;String, String&gt; options = new HashMap&lt;String, String&gt;();\noptions.put(&quot;header&quot;, &quot;true&quot;);\noptions.put(&quot;path&quot;, &quot;cars.csv&quot;);\noptions.put(&quot;inferSchema&quot;, &quot;true&quot;);\n\nDataFrame df = sqlContext.load(&quot;com.databricks.spark.csv&quot;, options);\n\nHashMap&lt;String, String&gt; saveOptions = new HashMap&lt;String, String&gt;();\nsaveOptions.put(&quot;header&quot;, &quot;true&quot;);\nsaveOptions.put(&quot;path&quot;, &quot;newcars.csv&quot;);\nsaveOptions.put(&quot;codec&quot;, &quot;org.apache.hadoop.io.compress.GzipCodec&quot;);\n\ndf.select(&quot;year&quot;, &quot;model&quot;).save(&quot;com.databricks.spark.csv&quot;, SaveMode.Overwrite,\n                                saveOptions);\"><pre><span class=\"pl-k\">import</span> <span class=\"pl-s1\">org</span>.<span class=\"pl-s1\">apache</span>.<span class=\"pl-s1\">spark</span>.<span class=\"pl-s1\">sql</span>.<span class=\"pl-s1\">SQLContext</span>;\n<span class=\"pl-k\">import</span> <span class=\"pl-s1\">org</span>.<span class=\"pl-s1\">apache</span>.<span class=\"pl-s1\">spark</span>.<span class=\"pl-s1\">sql</span>.<span class=\"pl-s1\">SaveMode</span>;\n\n<span class=\"pl-smi\">SQLContext</span> <span class=\"pl-s1\">sqlContext</span> = <span class=\"pl-k\">new</span> <span class=\"pl-smi\">SQLContext</span>(<span class=\"pl-s1\">sc</span>);\n\n<span class=\"pl-smi\">HashMap</span>&lt;<span class=\"pl-smi\">String</span>, <span class=\"pl-smi\">String</span>&gt; <span class=\"pl-s1\">options</span> = <span class=\"pl-k\">new</span> <span class=\"pl-smi\">HashMap</span>&lt;<span class=\"pl-smi\">String</span>, <span class=\"pl-smi\">String</span>&gt;();\n<span class=\"pl-s1\">options</span>.<span class=\"pl-en\">put</span>(<span class=\"pl-s\">\"header\"</span>, <span class=\"pl-s\">\"true\"</span>);\n<span class=\"pl-s1\">options</span>.<span class=\"pl-en\">put</span>(<span class=\"pl-s\">\"path\"</span>, <span class=\"pl-s\">\"cars.csv\"</span>);\n<span class=\"pl-s1\">options</span>.<span class=\"pl-en\">put</span>(<span class=\"pl-s\">\"inferSchema\"</span>, <span class=\"pl-s\">\"true\"</span>);\n\n<span class=\"pl-smi\">DataFrame</span> <span class=\"pl-s1\">df</span> = <span class=\"pl-s1\">sqlContext</span>.<span class=\"pl-en\">load</span>(<span class=\"pl-s\">\"com.databricks.spark.csv\"</span>, <span class=\"pl-s1\">options</span>);\n\n<span class=\"pl-smi\">HashMap</span>&lt;<span class=\"pl-smi\">String</span>, <span class=\"pl-smi\">String</span>&gt; <span class=\"pl-s1\">saveOptions</span> = <span class=\"pl-k\">new</span> <span class=\"pl-smi\">HashMap</span>&lt;<span class=\"pl-smi\">String</span>, <span class=\"pl-smi\">String</span>&gt;();\n<span class=\"pl-s1\">saveOptions</span>.<span class=\"pl-en\">put</span>(<span class=\"pl-s\">\"header\"</span>, <span class=\"pl-s\">\"true\"</span>);\n<span class=\"pl-s1\">saveOptions</span>.<span class=\"pl-en\">put</span>(<span class=\"pl-s\">\"path\"</span>, <span class=\"pl-s\">\"newcars.csv\"</span>);\n<span class=\"pl-s1\">saveOptions</span>.<span class=\"pl-en\">put</span>(<span class=\"pl-s\">\"codec\"</span>, <span class=\"pl-s\">\"org.apache.hadoop.io.compress.GzipCodec\"</span>);\n\n<span class=\"pl-s1\">df</span>.<span class=\"pl-en\">select</span>(<span class=\"pl-s\">\"year\"</span>, <span class=\"pl-s\">\"model\"</span>).<span class=\"pl-en\">save</span>(<span class=\"pl-s\">\"com.databricks.spark.csv\"</span>, <span class=\"pl-smi\">SaveMode</span>.<span class=\"pl-s1\">Overwrite</span>,\n                                <span class=\"pl-s1\">saveOptions</span>);</pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">Python API</h3><a id=\"user-content-python-api\" class=\"anchor\" aria-label=\"Permalink: Python API\" href=\"#python-api\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\"><strong>Spark 1.4+:</strong></p>\n<p dir=\"auto\">Automatically infer schema (data types), otherwise everything is assumed string:</p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\ndf = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('cars.csv')\ndf.select('year', 'model').write.format('com.databricks.spark.csv').save('newcars.csv')\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">pyspark</span>.<span class=\"pl-s1\">sql</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">SQLContext</span>\n<span class=\"pl-s1\">sqlContext</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">SQLContext</span>(<span class=\"pl-s1\">sc</span>)\n\n<span class=\"pl-s1\">df</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">sqlContext</span>.<span class=\"pl-s1\">read</span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s\">'com.databricks.spark.csv'</span>).<span class=\"pl-en\">options</span>(<span class=\"pl-s1\">header</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'true'</span>, <span class=\"pl-s1\">inferschema</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'true'</span>).<span class=\"pl-en\">load</span>(<span class=\"pl-s\">'cars.csv'</span>)\n<span class=\"pl-s1\">df</span>.<span class=\"pl-en\">select</span>(<span class=\"pl-s\">'year'</span>, <span class=\"pl-s\">'model'</span>).<span class=\"pl-s1\">write</span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s\">'com.databricks.spark.csv'</span>).<span class=\"pl-en\">save</span>(<span class=\"pl-s\">'newcars.csv'</span>)</pre></div>\n<p dir=\"auto\">You can manually specify schema:</p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"from pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\n\nsqlContext = SQLContext(sc)\ncustomSchema = StructType([ \\\n    StructField(&quot;year&quot;, IntegerType(), True), \\\n    StructField(&quot;make&quot;, StringType(), True), \\\n    StructField(&quot;model&quot;, StringType(), True), \\\n    StructField(&quot;comment&quot;, StringType(), True), \\\n    StructField(&quot;blank&quot;, StringType(), True)])\n\ndf = sqlContext.read \\\n    .format('com.databricks.spark.csv') \\\n    .options(header='true') \\\n    .load('cars.csv', schema = customSchema)\n\ndf.select('year', 'model').write \\\n    .format('com.databricks.spark.csv') \\\n    .save('newcars.csv')\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">pyspark</span>.<span class=\"pl-s1\">sql</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">SQLContext</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">pyspark</span>.<span class=\"pl-s1\">sql</span>.<span class=\"pl-s1\">types</span> <span class=\"pl-k\">import</span> <span class=\"pl-c1\">*</span>\n\n<span class=\"pl-s1\">sqlContext</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">SQLContext</span>(<span class=\"pl-s1\">sc</span>)\n<span class=\"pl-s1\">customSchema</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">StructType</span>([ \\\n    <span class=\"pl-v\">StructField</span>(<span class=\"pl-s\">\"year\"</span>, <span class=\"pl-v\">IntegerType</span>(), <span class=\"pl-c1\">True</span>), \\\n    <span class=\"pl-v\">StructField</span>(<span class=\"pl-s\">\"make\"</span>, <span class=\"pl-v\">StringType</span>(), <span class=\"pl-c1\">True</span>), \\\n    <span class=\"pl-v\">StructField</span>(<span class=\"pl-s\">\"model\"</span>, <span class=\"pl-v\">StringType</span>(), <span class=\"pl-c1\">True</span>), \\\n    <span class=\"pl-v\">StructField</span>(<span class=\"pl-s\">\"comment\"</span>, <span class=\"pl-v\">StringType</span>(), <span class=\"pl-c1\">True</span>), \\\n    <span class=\"pl-v\">StructField</span>(<span class=\"pl-s\">\"blank\"</span>, <span class=\"pl-v\">StringType</span>(), <span class=\"pl-c1\">True</span>)])\n\n<span class=\"pl-s1\">df</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">sqlContext</span>.<span class=\"pl-s1\">read</span> \\\n    .<span class=\"pl-en\">format</span>(<span class=\"pl-s\">'com.databricks.spark.csv'</span>) \\\n    .<span class=\"pl-en\">options</span>(<span class=\"pl-s1\">header</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'true'</span>) \\\n    .<span class=\"pl-en\">load</span>(<span class=\"pl-s\">'cars.csv'</span>, <span class=\"pl-s1\">schema</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">customSchema</span>)\n\n<span class=\"pl-s1\">df</span>.<span class=\"pl-en\">select</span>(<span class=\"pl-s\">'year'</span>, <span class=\"pl-s\">'model'</span>).<span class=\"pl-s1\">write</span> \\\n    .<span class=\"pl-en\">format</span>(<span class=\"pl-s\">'com.databricks.spark.csv'</span>) \\\n    .<span class=\"pl-en\">save</span>(<span class=\"pl-s\">'newcars.csv'</span>)</pre></div>\n<p dir=\"auto\">You can save with compressed output:</p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\ndf = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('cars.csv')\ndf.select('year', 'model').write.format('com.databricks.spark.csv').options(codec=&quot;org.apache.hadoop.io.compress.GzipCodec&quot;).save('newcars.csv')\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">pyspark</span>.<span class=\"pl-s1\">sql</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">SQLContext</span>\n<span class=\"pl-s1\">sqlContext</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">SQLContext</span>(<span class=\"pl-s1\">sc</span>)\n\n<span class=\"pl-s1\">df</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">sqlContext</span>.<span class=\"pl-s1\">read</span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s\">'com.databricks.spark.csv'</span>).<span class=\"pl-en\">options</span>(<span class=\"pl-s1\">header</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'true'</span>, <span class=\"pl-s1\">inferschema</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">'true'</span>).<span class=\"pl-en\">load</span>(<span class=\"pl-s\">'cars.csv'</span>)\n<span class=\"pl-s1\">df</span>.<span class=\"pl-en\">select</span>(<span class=\"pl-s\">'year'</span>, <span class=\"pl-s\">'model'</span>).<span class=\"pl-s1\">write</span>.<span class=\"pl-en\">format</span>(<span class=\"pl-s\">'com.databricks.spark.csv'</span>).<span class=\"pl-en\">options</span>(<span class=\"pl-s1\">codec</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"org.apache.hadoop.io.compress.GzipCodec\"</span>).<span class=\"pl-en\">save</span>(<span class=\"pl-s\">'newcars.csv'</span>)</pre></div>\n<p dir=\"auto\"><strong>Spark 1.3:</strong></p>\n<p dir=\"auto\">Automatically infer schema (data types), otherwise everything is assumed string:</p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\ndf = sqlContext.load(source=&quot;com.databricks.spark.csv&quot;, header = 'true', inferSchema = 'true', path = 'cars.csv')\ndf.select('year', 'model').save('newcars.csv', 'com.databricks.spark.csv')\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">pyspark</span>.<span class=\"pl-s1\">sql</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">SQLContext</span>\n<span class=\"pl-s1\">sqlContext</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">SQLContext</span>(<span class=\"pl-s1\">sc</span>)\n\n<span class=\"pl-s1\">df</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">sqlContext</span>.<span class=\"pl-en\">load</span>(<span class=\"pl-s1\">source</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"com.databricks.spark.csv\"</span>, <span class=\"pl-s1\">header</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">'true'</span>, <span class=\"pl-s1\">inferSchema</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">'true'</span>, <span class=\"pl-s1\">path</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">'cars.csv'</span>)\n<span class=\"pl-s1\">df</span>.<span class=\"pl-en\">select</span>(<span class=\"pl-s\">'year'</span>, <span class=\"pl-s\">'model'</span>).<span class=\"pl-en\">save</span>(<span class=\"pl-s\">'newcars.csv'</span>, <span class=\"pl-s\">'com.databricks.spark.csv'</span>)</pre></div>\n<p dir=\"auto\">You can manually specify schema:</p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"from pyspark.sql import SQLContext\nfrom pyspark.sql.types import *\n\nsqlContext = SQLContext(sc)\ncustomSchema = StructType([ \\\n    StructField(&quot;year&quot;, IntegerType(), True), \\\n    StructField(&quot;make&quot;, StringType(), True), \\\n    StructField(&quot;model&quot;, StringType(), True), \\\n    StructField(&quot;comment&quot;, StringType(), True), \\\n    StructField(&quot;blank&quot;, StringType(), True)])\n\ndf = sqlContext.load(source=&quot;com.databricks.spark.csv&quot;, header = 'true', schema = customSchema, path = 'cars.csv')\ndf.select('year', 'model').save('newcars.csv', 'com.databricks.spark.csv')\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">pyspark</span>.<span class=\"pl-s1\">sql</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">SQLContext</span>\n<span class=\"pl-k\">from</span> <span class=\"pl-s1\">pyspark</span>.<span class=\"pl-s1\">sql</span>.<span class=\"pl-s1\">types</span> <span class=\"pl-k\">import</span> <span class=\"pl-c1\">*</span>\n\n<span class=\"pl-s1\">sqlContext</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">SQLContext</span>(<span class=\"pl-s1\">sc</span>)\n<span class=\"pl-s1\">customSchema</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">StructType</span>([ \\\n    <span class=\"pl-v\">StructField</span>(<span class=\"pl-s\">\"year\"</span>, <span class=\"pl-v\">IntegerType</span>(), <span class=\"pl-c1\">True</span>), \\\n    <span class=\"pl-v\">StructField</span>(<span class=\"pl-s\">\"make\"</span>, <span class=\"pl-v\">StringType</span>(), <span class=\"pl-c1\">True</span>), \\\n    <span class=\"pl-v\">StructField</span>(<span class=\"pl-s\">\"model\"</span>, <span class=\"pl-v\">StringType</span>(), <span class=\"pl-c1\">True</span>), \\\n    <span class=\"pl-v\">StructField</span>(<span class=\"pl-s\">\"comment\"</span>, <span class=\"pl-v\">StringType</span>(), <span class=\"pl-c1\">True</span>), \\\n    <span class=\"pl-v\">StructField</span>(<span class=\"pl-s\">\"blank\"</span>, <span class=\"pl-v\">StringType</span>(), <span class=\"pl-c1\">True</span>)])\n\n<span class=\"pl-s1\">df</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">sqlContext</span>.<span class=\"pl-en\">load</span>(<span class=\"pl-s1\">source</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"com.databricks.spark.csv\"</span>, <span class=\"pl-s1\">header</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">'true'</span>, <span class=\"pl-s1\">schema</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">customSchema</span>, <span class=\"pl-s1\">path</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">'cars.csv'</span>)\n<span class=\"pl-s1\">df</span>.<span class=\"pl-en\">select</span>(<span class=\"pl-s\">'year'</span>, <span class=\"pl-s\">'model'</span>).<span class=\"pl-en\">save</span>(<span class=\"pl-s\">'newcars.csv'</span>, <span class=\"pl-s\">'com.databricks.spark.csv'</span>)</pre></div>\n<p dir=\"auto\">You can save with compressed output:</p>\n<div class=\"highlight highlight-source-python notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"from pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\ndf = sqlContext.load(source=&quot;com.databricks.spark.csv&quot;, header = 'true', inferSchema = 'true', path = 'cars.csv')\ndf.select('year', 'model').save('newcars.csv', 'com.databricks.spark.csv', codec=&quot;org.apache.hadoop.io.compress.GzipCodec&quot;)\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-s1\">pyspark</span>.<span class=\"pl-s1\">sql</span> <span class=\"pl-k\">import</span> <span class=\"pl-v\">SQLContext</span>\n<span class=\"pl-s1\">sqlContext</span> <span class=\"pl-c1\">=</span> <span class=\"pl-v\">SQLContext</span>(<span class=\"pl-s1\">sc</span>)\n\n<span class=\"pl-s1\">df</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s1\">sqlContext</span>.<span class=\"pl-en\">load</span>(<span class=\"pl-s1\">source</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"com.databricks.spark.csv\"</span>, <span class=\"pl-s1\">header</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">'true'</span>, <span class=\"pl-s1\">inferSchema</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">'true'</span>, <span class=\"pl-s1\">path</span> <span class=\"pl-c1\">=</span> <span class=\"pl-s\">'cars.csv'</span>)\n<span class=\"pl-s1\">df</span>.<span class=\"pl-en\">select</span>(<span class=\"pl-s\">'year'</span>, <span class=\"pl-s\">'model'</span>).<span class=\"pl-en\">save</span>(<span class=\"pl-s\">'newcars.csv'</span>, <span class=\"pl-s\">'com.databricks.spark.csv'</span>, <span class=\"pl-s1\">codec</span><span class=\"pl-c1\">=</span><span class=\"pl-s\">\"org.apache.hadoop.io.compress.GzipCodec\"</span>)</pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h3 class=\"heading-element\" dir=\"auto\">R API</h3><a id=\"user-content-r-api\" class=\"anchor\" aria-label=\"Permalink: R API\" href=\"#r-api\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\"><strong>Spark 1.4+:</strong></p>\n<p dir=\"auto\">Automatically infer schema (data types), otherwise everything is assumed string:</p>\n<div class=\"highlight highlight-source-r notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"library(SparkR)\n\nSys.setenv('SPARKR_SUBMIT_ARGS'='&quot;--packages&quot; &quot;com.databricks:spark-csv_2.10:1.4.0&quot; &quot;sparkr-shell&quot;')\nsqlContext &lt;- sparkRSQL.init(sc)\n\ndf &lt;- read.df(sqlContext, &quot;cars.csv&quot;, source = &quot;com.databricks.spark.csv&quot;, inferSchema = &quot;true&quot;)\n\nwrite.df(df, &quot;newcars.csv&quot;, &quot;com.databricks.spark.csv&quot;, &quot;overwrite&quot;)\"><pre>library(<span class=\"pl-smi\">SparkR</span>)\n\nSys.setenv(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>SPARKR_SUBMIT_ARGS<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>\"--packages\" \"com.databricks:spark-csv_2.10:1.4.0\" \"sparkr-shell\"<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-smi\">sqlContext</span> <span class=\"pl-k\">&lt;-</span> sparkRSQL.init(<span class=\"pl-smi\">sc</span>)\n\n<span class=\"pl-smi\">df</span> <span class=\"pl-k\">&lt;-</span> read.df(<span class=\"pl-smi\">sqlContext</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cars.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">source</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">inferSchema</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>)\n\nwrite.df(<span class=\"pl-smi\">df</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>newcars.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>overwrite<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p dir=\"auto\">You can manually specify schema:</p>\n<div class=\"highlight highlight-source-r notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"library(SparkR)\n\nSys.setenv('SPARKR_SUBMIT_ARGS'='&quot;--packages&quot; &quot;com.databricks:spark-csv_2.10:1.4.0&quot; &quot;sparkr-shell&quot;')\nsqlContext &lt;- sparkRSQL.init(sc)\ncustomSchema &lt;- structType(\n    structField(&quot;year&quot;, &quot;integer&quot;),\n    structField(&quot;make&quot;, &quot;string&quot;),\n    structField(&quot;model&quot;, &quot;string&quot;),\n    structField(&quot;comment&quot;, &quot;string&quot;),\n    structField(&quot;blank&quot;, &quot;string&quot;))\n\ndf &lt;- read.df(sqlContext, &quot;cars.csv&quot;, source = &quot;com.databricks.spark.csv&quot;, schema = customSchema)\n\nwrite.df(df, &quot;newcars.csv&quot;, &quot;com.databricks.spark.csv&quot;, &quot;overwrite&quot;)\"><pre>library(<span class=\"pl-smi\">SparkR</span>)\n\nSys.setenv(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>SPARKR_SUBMIT_ARGS<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>\"--packages\" \"com.databricks:spark-csv_2.10:1.4.0\" \"sparkr-shell\"<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-smi\">sqlContext</span> <span class=\"pl-k\">&lt;-</span> sparkRSQL.init(<span class=\"pl-smi\">sc</span>)\n<span class=\"pl-smi\">customSchema</span> <span class=\"pl-k\">&lt;-</span> structType(\n    structField(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>year<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>integer<span class=\"pl-pds\">\"</span></span>),\n    structField(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>make<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>string<span class=\"pl-pds\">\"</span></span>),\n    structField(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>model<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>string<span class=\"pl-pds\">\"</span></span>),\n    structField(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>comment<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>string<span class=\"pl-pds\">\"</span></span>),\n    structField(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>blank<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>string<span class=\"pl-pds\">\"</span></span>))\n\n<span class=\"pl-smi\">df</span> <span class=\"pl-k\">&lt;-</span> read.df(<span class=\"pl-smi\">sqlContext</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cars.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">source</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">schema</span> <span class=\"pl-k\">=</span> <span class=\"pl-smi\">customSchema</span>)\n\nwrite.df(<span class=\"pl-smi\">df</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>newcars.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>overwrite<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p dir=\"auto\">You can save with compressed output:</p>\n<div class=\"highlight highlight-source-r notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"library(SparkR)\n\nSys.setenv('SPARKR_SUBMIT_ARGS'='&quot;--packages&quot; &quot;com.databricks:spark-csv_2.10:1.4.0&quot; &quot;sparkr-shell&quot;')\nsqlContext &lt;- sparkRSQL.init(sc)\n\ndf &lt;- read.df(sqlContext, &quot;cars.csv&quot;, source = &quot;com.databricks.spark.csv&quot;, inferSchema = &quot;true&quot;)\n\nwrite.df(df, &quot;newcars.csv&quot;, &quot;com.databricks.spark.csv&quot;, &quot;overwrite&quot;, codec=&quot;org.apache.hadoop.io.compress.GzipCodec&quot;)\"><pre>library(<span class=\"pl-smi\">SparkR</span>)\n\nSys.setenv(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>SPARKR_SUBMIT_ARGS<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>\"--packages\" \"com.databricks:spark-csv_2.10:1.4.0\" \"sparkr-shell\"<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-smi\">sqlContext</span> <span class=\"pl-k\">&lt;-</span> sparkRSQL.init(<span class=\"pl-smi\">sc</span>)\n\n<span class=\"pl-smi\">df</span> <span class=\"pl-k\">&lt;-</span> read.df(<span class=\"pl-smi\">sqlContext</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cars.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">source</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">inferSchema</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>true<span class=\"pl-pds\">\"</span></span>)\n\nwrite.df(<span class=\"pl-smi\">df</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>newcars.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.databricks.spark.csv<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>overwrite<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">codec</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>org.apache.hadoop.io.compress.GzipCodec<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<div class=\"markdown-heading\" dir=\"auto\"><h2 class=\"heading-element\" dir=\"auto\">Building From Source</h2><a id=\"user-content-building-from-source\" class=\"anchor\" aria-label=\"Permalink: Building From Source\" href=\"#building-from-source\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"></path></svg></a></div>\n<p dir=\"auto\">This library is built with <a href=\"http://www.scala-sbt.org/0.13/docs/Command-Line-Reference.html\" rel=\"nofollow\">SBT</a>, which is automatically downloaded by the included shell script. To build a JAR file simply run <code>sbt/sbt package</code> from the project root. The build configuration includes support for both Scala 2.10 and 2.11.</p>\n</article></div>",
    "contributors" : [
      {
        "login" : "falaki",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/512364?v=4",
        "url" : "https://github.com/falaki",
        "contributions" : 155
      },
      {
        "login" : "dtpeacock",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/10317254?v=4",
        "url" : "https://github.com/dtpeacock",
        "contributions" : 19
      },
      {
        "login" : "HyukjinKwon",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/6477701?v=4",
        "url" : "https://github.com/HyukjinKwon",
        "contributions" : 15
      },
      {
        "login" : "JoshRosen",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/50748?v=4",
        "url" : "https://github.com/JoshRosen",
        "contributions" : 11
      },
      {
        "login" : "jaley",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/343449?v=4",
        "url" : "https://github.com/jaley",
        "contributions" : 7
      },
      {
        "login" : "pashields",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/170202?v=4",
        "url" : "https://github.com/pashields",
        "contributions" : 6
      },
      {
        "login" : "huangjs",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/27528?v=4",
        "url" : "https://github.com/huangjs",
        "contributions" : 4
      },
      {
        "login" : "rxin",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/323388?v=4",
        "url" : "https://github.com/rxin",
        "contributions" : 4
      },
      {
        "login" : "vlyubin",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/3303322?v=4",
        "url" : "https://github.com/vlyubin",
        "contributions" : 4
      },
      {
        "login" : "brkyvz",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/5243515?v=4",
        "url" : "https://github.com/brkyvz",
        "contributions" : 3
      },
      {
        "login" : "dennishuo",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/7410123?v=4",
        "url" : "https://github.com/dennishuo",
        "contributions" : 2
      },
      {
        "login" : "mengxr",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/829644?v=4",
        "url" : "https://github.com/mengxr",
        "contributions" : 2
      },
      {
        "login" : "tanwanirahul",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/3842010?v=4",
        "url" : "https://github.com/tanwanirahul",
        "contributions" : 2
      },
      {
        "login" : "gasparms",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/5221342?v=4",
        "url" : "https://github.com/gasparms",
        "contributions" : 1
      },
      {
        "login" : "chie8842",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/5276569?v=4",
        "url" : "https://github.com/chie8842",
        "contributions" : 1
      },
      {
        "login" : "yucheng1992",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/8794412?v=4",
        "url" : "https://github.com/yucheng1992",
        "contributions" : 1
      },
      {
        "login" : "yhuai",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/2072857?v=4",
        "url" : "https://github.com/yhuai",
        "contributions" : 1
      },
      {
        "login" : "tobithiel",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/956860?v=4",
        "url" : "https://github.com/tobithiel",
        "contributions" : 1
      },
      {
        "login" : "sryza",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/654855?v=4",
        "url" : "https://github.com/sryza",
        "contributions" : 1
      },
      {
        "login" : "cvengros",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/1019202?v=4",
        "url" : "https://github.com/cvengros",
        "contributions" : 1
      },
      {
        "login" : "petro-rudenko",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/1121987?v=4",
        "url" : "https://github.com/petro-rudenko",
        "contributions" : 1
      },
      {
        "login" : "Mayank-Shete",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/10369535?v=4",
        "url" : "https://github.com/Mayank-Shete",
        "contributions" : 1
      },
      {
        "login" : "msperlich",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/1402211?v=4",
        "url" : "https://github.com/msperlich",
        "contributions" : 1
      },
      {
        "login" : "MarkRijckenberg",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/3747018?v=4",
        "url" : "https://github.com/MarkRijckenberg",
        "contributions" : 1
      },
      {
        "login" : "MarcinKosinski",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/6773454?v=4",
        "url" : "https://github.com/MarcinKosinski",
        "contributions" : 1
      },
      {
        "login" : "kmader",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/116120?v=4",
        "url" : "https://github.com/kmader",
        "contributions" : 1
      },
      {
        "login" : "digigek",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/5501783?v=4",
        "url" : "https://github.com/digigek",
        "contributions" : 1
      },
      {
        "login" : "saurfang",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/4317392?v=4",
        "url" : "https://github.com/saurfang",
        "contributions" : 1
      },
      {
        "login" : "lebigot",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/5793?v=4",
        "url" : "https://github.com/lebigot",
        "contributions" : 1
      },
      {
        "login" : "darabos",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/1268018?v=4",
        "url" : "https://github.com/darabos",
        "contributions" : 1
      },
      {
        "login" : "andy327",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/2125475?v=4",
        "url" : "https://github.com/andy327",
        "contributions" : 1
      },
      {
        "login" : "akirakw",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/678656?v=4",
        "url" : "https://github.com/akirakw",
        "contributions" : 1
      }
    ],
    "commits" : 253,
    "topics" : [
    ],
    "contributingGuide" : null,
    "codeOfConduct" : null,
    "openIssues" : [
    ],
    "scalaPercentage" : 80,
    "license" : "Apache-2.0",
    "commitActivity" : [
    ]
  },
  "settings" : {
    "preferStableVersion" : true,
    "defaultArtifact" : "spark-csv",
    "customScalaDoc" : null,
    "documentationLinks" : [
    ],
    "contributorsWanted" : false,
    "deprecatedArtifacts" : [
    ],
    "cliArtifacts" : [
    ],
    "category" : "data-sources-and-connectors",
    "chatroom" : null
  }
}