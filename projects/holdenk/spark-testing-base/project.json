{
  "organization" : "holdenk",
  "repository" : "spark-testing-base",
  "creationDate" : 1424054310967,
  "githubStatus" : {
    "Ok" : {
      "updateDate" : 1644309160059
    }
  },
  "githubInfo" : {
    "homepage" : null,
    "description" : "Base classes to use when writing tests with Spark",
    "logo" : "https://avatars.githubusercontent.com/u/59893?v=4",
    "stars" : 1324,
    "forks" : 344,
    "watchers" : 78,
    "issues" : 109,
    "creationDate" : 1422656639000,
    "readme" : "<div id=\"readme\" class=\"md\" data-path=\"README.md\"><article class=\"markdown-body entry-content container-lg\" itemprop=\"text\"><p dir=\"auto\"><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/holdenk/spark-testing-base/actions/workflows/github-actions-basic.yml/badge.svg?branch=master\"><img src=\"https://github.com/holdenk/spark-testing-base/actions/workflows/github-actions-basic.yml/badge.svg?branch=master\" alt=\"build status\" style=\"max-width: 100%;\"></a></p>\n<h1 dir=\"auto\"><a id=\"user-content-spark-testing-base\" class=\"anchor\" aria-hidden=\"true\" href=\"#spark-testing-base\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>spark-testing-base</h1>\n<p dir=\"auto\">Base classes to use when writing tests with Spark.</p>\n<h2 dir=\"auto\"><a id=\"user-content-why\" class=\"anchor\" aria-hidden=\"true\" href=\"#why\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Why?</h2>\n<p dir=\"auto\">You've written an awesome program in Spark and now its time to write some tests. Only you find yourself writing the code to setup and tear down local mode Spark in between each suite and you say to your self:\nThis is not my beautiful code.</p>\n<h2 dir=\"auto\"><a id=\"user-content-how\" class=\"anchor\" aria-hidden=\"true\" href=\"#how\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>How?</h2>\n<p dir=\"auto\">So you include com.holdenkarau.spark-testing-base [spark_version]_1.0.0 and extend one of the classes and write some simple tests instead.  For example to include this in a project using Spark 3.0.0:</p>\n<div class=\"highlight highlight-source-scala position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"&quot;com.holdenkarau&quot; %% &quot;spark-testing-base&quot; % &quot;3.0.0_1.0.0&quot; % &quot;test&quot;\"><pre><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.holdenkarau<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%%</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>spark-testing-base<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>3.0.0_1.0.0<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test<span class=\"pl-pds\">\"</span></span></pre></div>\n<p dir=\"auto\">or</p>\n<div class=\"snippet-clipboard-content position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"&lt;dependency&gt;\n    &lt;groupId&gt;com.holdenkarau&lt;/groupId&gt;\n    &lt;artifactId&gt;spark-testing-base_2.11&lt;/artifactId&gt;\n    &lt;version&gt;${spark.version}_0.11.0&lt;/version&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\"><pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.holdenkarau&lt;/groupId&gt;\n    &lt;artifactId&gt;spark-testing-base_2.11&lt;/artifactId&gt;\n    &lt;version&gt;${spark.version}_0.11.0&lt;/version&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n</code></pre></div>\n<p dir=\"auto\">If you'd like to use Kafka related features you need to include this artefact to your dependencies as well:</p>\n<div class=\"highlight highlight-source-scala position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"&quot;com.holdenkarau&quot; %% &quot;spark-testing-kafka-0_8&quot; % &quot;3.0.0_0.14.0&quot; % &quot;test&quot;\"><pre><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>com.holdenkarau<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%%</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>spark-testing-kafka-0_8<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>3.0.0_0.14.0<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test<span class=\"pl-pds\">\"</span></span></pre></div>\n<p dir=\"auto\">or</p>\n<div class=\"snippet-clipboard-content position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"&lt;dependency&gt;\n    &lt;groupId&gt;com.holdenkarau&lt;/groupId&gt;\n    &lt;artifactId&gt;spark-testing-kafka-0_8_2.11&lt;/artifactId&gt;\n    &lt;version&gt;${spark.version}_0.14.0&lt;/version&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\"><pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;com.holdenkarau&lt;/groupId&gt;\n    &lt;artifactId&gt;spark-testing-kafka-0_8_2.11&lt;/artifactId&gt;\n    &lt;version&gt;${spark.version}_0.14.0&lt;/version&gt;\n    &lt;scope&gt;test&lt;/scope&gt;\n&lt;/dependency&gt;\n</code></pre></div>\n<p dir=\"auto\">Currently the Kafka dependency is <em>only</em> built for Scala 2.11.</p>\n<p dir=\"auto\">How to use it inside your code? have a look at the <a href=\"https://github.com/holdenk/spark-testing-base/wiki\">wiki</a> page.</p>\n<p dir=\"auto\">The <a href=\"https://mvnrepository.com/artifact/com.holdenkarau\" rel=\"nofollow\">Maven repositories page for spark-testing-base</a> lists the releases available.</p>\n<p dir=\"auto\">The Python package of spark-testing-base is available via:</p>\n<ul dir=\"auto\">\n<li>PyPI: <a href=\"https://pypi.org/project/spark-testing-base/\" rel=\"nofollow\">https://pypi.org/project/spark-testing-base/</a>, e.g. <code>pip install spark-testing-base</code></li>\n<li>Conda: <a href=\"https://anaconda.org/conda-forge/spark-testing-base\" rel=\"nofollow\">https://anaconda.org/conda-forge/spark-testing-base</a>, e.g. <code>conda install -c conda-forge spark-testing-base</code></li>\n</ul>\n<h2 dir=\"auto\"><a id=\"user-content-minimum-memory-requirements-and-ooms\" class=\"anchor\" aria-hidden=\"true\" href=\"#minimum-memory-requirements-and-ooms\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Minimum Memory Requirements and OOMs</h2>\n<p dir=\"auto\">The default SBT testing java options are too small to support running many of the tests due to the need to launch Spark in local mode. To increase the amount of memory in a build.sbt file you can add:</p>\n<div class=\"highlight highlight-source-scala position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"fork in Test := true\njavaOptions ++= Seq(&quot;-Xms512M&quot;, &quot;-Xmx2048M&quot;, &quot;-XX:MaxPermSize=2048M&quot;, &quot;-XX:+CMSClassUnloadingEnabled&quot;)\"><pre>fork in <span class=\"pl-en\">Test</span> <span class=\"pl-k\">:</span><span class=\"pl-k\">=</span> <span class=\"pl-c1\">true</span>\njavaOptions <span class=\"pl-k\">++</span><span class=\"pl-k\">=</span> <span class=\"pl-en\">Seq</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>-Xms512M<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>-Xmx2048M<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>-XX:MaxPermSize=2048M<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>-XX:+CMSClassUnloadingEnabled<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p dir=\"auto\">If using surefire you can add:</p>\n<div class=\"snippet-clipboard-content position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"&lt;argLine&gt;-Xmx2048m -XX:MaxPermSize=2048m&lt;/argLine&gt;\"><pre><code>&lt;argLine&gt;-Xmx2048m -XX:MaxPermSize=2048m&lt;/argLine&gt;\n</code></pre></div>\n<p dir=\"auto\">Note: the specific memory values are examples only (and the values used to run spark-testing-base's own tests).</p>\n<h2 dir=\"auto\"><a id=\"user-content-special-considerations\" class=\"anchor\" aria-hidden=\"true\" href=\"#special-considerations\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Special considerations</h2>\n<p dir=\"auto\">Make sure to disable parallel execution.</p>\n<p dir=\"auto\">In sbt you can add:</p>\n<div class=\"highlight highlight-source-scala position-relative overflow-auto\" data-snippet-clipboard-copy-content=\"parallelExecution in Test := false\"><pre>parallelExecution in <span class=\"pl-en\">Test</span> <span class=\"pl-k\">:</span><span class=\"pl-k\">=</span> <span class=\"pl-c1\">false</span></pre></div>\n<p dir=\"auto\">In surefire make sure that forkCount is set to 1 and reuseForks is true.</p>\n<h2 dir=\"auto\"><a id=\"user-content-where-is-this-from\" class=\"anchor\" aria-hidden=\"true\" href=\"#where-is-this-from\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>Where is this from?</h2>\n<p dir=\"auto\">Some of this code is a stripped down version of the test suite bases that are in Apache Spark but are not accessible. Other parts are also inspired by sscheck (scalacheck generators for Spark).</p>\n<p dir=\"auto\">Other parts of this are implemented on top of the test suite bases to make your life even easier.</p>\n<h2 dir=\"auto\"><a id=\"user-content-how-do-i-build-this\" class=\"anchor\" aria-hidden=\"true\" href=\"#how-do-i-build-this\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>How do I build this?</h2>\n<p dir=\"auto\">This project is built with sbt.</p>\n<h2 dir=\"auto\"><a id=\"user-content-what-are-some-other-options\" class=\"anchor\" aria-hidden=\"true\" href=\"#what-are-some-other-options\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a>What are some other options?</h2>\n<p dir=\"auto\">While we hope you choose our library, <a href=\"https://github.com/juanrh/sscheck\">https://github.com/juanrh/sscheck</a> , <a href=\"https://github.com/hammerlab/spark-tests\">https://github.com/hammerlab/spark-tests</a> , <a href=\"https://github.com/wdm0006/DummyRDD\">https://github.com/wdm0006/DummyRDD</a> , and more <a href=\"https://www.google.com/search?q=python+spark+testing+libraries\" rel=\"nofollow\">https://www.google.com/search?q=python+spark+testing+libraries</a> exist as options.</p>\n<h2 dir=\"auto\"><a id=\"user-content-release-notes\" class=\"anchor\" aria-hidden=\"true\" href=\"#release-notes\"><svg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"><path fill-rule=\"evenodd\" d=\"M7.775 3.275a.75.75 0 001.06 1.06l1.25-1.25a2 2 0 112.83 2.83l-2.5 2.5a2 2 0 01-2.83 0 .75.75 0 00-1.06 1.06 3.5 3.5 0 004.95 0l2.5-2.5a3.5 3.5 0 00-4.95-4.95l-1.25 1.25zm-4.69 9.64a2 2 0 010-2.83l2.5-2.5a2 2 0 012.83 0 .75.75 0 001.06-1.06 3.5 3.5 0 00-4.95 0l-2.5 2.5a3.5 3.5 0 004.95 4.95l1.25-1.25a.75.75 0 00-1.06-1.06l-1.25 1.25a2 2 0 01-2.83 0z\"></path></svg></a><a href=\"RELEASE_NOTES.md\">Release Notes</a></h2>\n</article></div>",
    "contributors" : [
      {
        "login" : "holdenk",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/59893?v=4",
        "url" : "https://api.github.com/users/holdenk",
        "contributions" : 491
      },
      {
        "login" : "mahmoudhanafy",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/3827156?v=4",
        "url" : "https://api.github.com/users/mahmoudhanafy",
        "contributions" : 79
      },
      {
        "login" : "MrPowers",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/2722395?v=4",
        "url" : "https://api.github.com/users/MrPowers",
        "contributions" : 8
      },
      {
        "login" : "rylanhalteman",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/2278079?v=4",
        "url" : "https://api.github.com/users/rylanhalteman",
        "contributions" : 7
      },
      {
        "login" : "zouzias",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/1871144?v=4",
        "url" : "https://api.github.com/users/zouzias",
        "contributions" : 6
      },
      {
        "login" : "kaatzee",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/2965012?v=4",
        "url" : "https://api.github.com/users/kaatzee",
        "contributions" : 5
      },
      {
        "login" : "limansky",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/111185?v=4",
        "url" : "https://api.github.com/users/limansky",
        "contributions" : 5
      },
      {
        "login" : "bryanyang0528",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/5171607?v=4",
        "url" : "https://api.github.com/users/bryanyang0528",
        "contributions" : 2
      },
      {
        "login" : "dbast",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/2790401?v=4",
        "url" : "https://api.github.com/users/dbast",
        "contributions" : 2
      },
      {
        "login" : "hgiddens",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/198783?v=4",
        "url" : "https://api.github.com/users/hgiddens",
        "contributions" : 2
      },
      {
        "login" : "nightscape",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/35170?v=4",
        "url" : "https://api.github.com/users/nightscape",
        "contributions" : 2
      },
      {
        "login" : "ponkin",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/323904?v=4",
        "url" : "https://api.github.com/users/ponkin",
        "contributions" : 1
      },
      {
        "login" : "borisclemencon",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/10812246?v=4",
        "url" : "https://api.github.com/users/borisclemencon",
        "contributions" : 1
      },
      {
        "login" : "bryanv",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/113546?v=4",
        "url" : "https://api.github.com/users/bryanv",
        "contributions" : 1
      },
      {
        "login" : "brkyvz",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/5243515?v=4",
        "url" : "https://api.github.com/users/brkyvz",
        "contributions" : 1
      },
      {
        "login" : "dmvieira",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/1108613?v=4",
        "url" : "https://api.github.com/users/dmvieira",
        "contributions" : 1
      },
      {
        "login" : "lunaticare",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/947471?v=4",
        "url" : "https://api.github.com/users/lunaticare",
        "contributions" : 1
      },
      {
        "login" : "felipefzdz",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/369379?v=4",
        "url" : "https://api.github.com/users/felipefzdz",
        "contributions" : 1
      },
      {
        "login" : "felipehummel",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/21449?v=4",
        "url" : "https://api.github.com/users/felipehummel",
        "contributions" : 1
      },
      {
        "login" : "gneotux",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/1918857?v=4",
        "url" : "https://api.github.com/users/gneotux",
        "contributions" : 1
      },
      {
        "login" : "helfper",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/2981969?v=4",
        "url" : "https://api.github.com/users/helfper",
        "contributions" : 1
      },
      {
        "login" : "falloutdurham",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/97861?v=4",
        "url" : "https://api.github.com/users/falloutdurham",
        "contributions" : 1
      },
      {
        "login" : "jackcviers",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/660372?v=4",
        "url" : "https://api.github.com/users/jackcviers",
        "contributions" : 1
      },
      {
        "login" : "jerrypnz",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/292028?v=4",
        "url" : "https://api.github.com/users/jerrypnz",
        "contributions" : 1
      },
      {
        "login" : "JoshRosen",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/50748?v=4",
        "url" : "https://api.github.com/users/JoshRosen",
        "contributions" : 1
      },
      {
        "login" : "juanrh",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/895341?v=4",
        "url" : "https://api.github.com/users/juanrh",
        "contributions" : 1
      },
      {
        "login" : "Veske",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/5602603?v=4",
        "url" : "https://api.github.com/users/Veske",
        "contributions" : 1
      },
      {
        "login" : "markdessain",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/230477?v=4",
        "url" : "https://api.github.com/users/markdessain",
        "contributions" : 1
      },
      {
        "login" : "morazow",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/916295?v=4",
        "url" : "https://api.github.com/users/morazow",
        "contributions" : 1
      },
      {
        "login" : "sgt",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/5119?v=4",
        "url" : "https://api.github.com/users/sgt",
        "contributions" : 1
      },
      {
        "login" : "ac27182",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/42341528?v=4",
        "url" : "https://api.github.com/users/ac27182",
        "contributions" : 1
      },
      {
        "login" : "chiefmanc",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/11571352?v=4",
        "url" : "https://api.github.com/users/chiefmanc",
        "contributions" : 1
      },
      {
        "login" : "mandoz",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/1105556?v=4",
        "url" : "https://api.github.com/users/mandoz",
        "contributions" : 1
      },
      {
        "login" : "pchundi",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/4728134?v=4",
        "url" : "https://api.github.com/users/pchundi",
        "contributions" : 1
      },
      {
        "login" : "rgarciate",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/18609971?v=4",
        "url" : "https://api.github.com/users/rgarciate",
        "contributions" : 1
      },
      {
        "login" : "siklosid",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/11415774?v=4",
        "url" : "https://api.github.com/users/siklosid",
        "contributions" : 1
      },
      {
        "login" : "smadarasmi",
        "avatarUrl" : "https://avatars.githubusercontent.com/u/23423749?v=4",
        "url" : "https://api.github.com/users/smadarasmi",
        "contributions" : 1
      }
    ],
    "commits" : 635,
    "topics" : [
    ],
    "contributingGuide" : null,
    "codeOfConduct" : null,
    "chatroom" : null,
    "openIssues" : [
      {
        "number" : 348,
        "title" : "ZIO-test support ? ",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/348"
      },
      {
        "number" : 347,
        "title" : "Add cross support for Scala 2.13",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/347"
      },
      {
        "number" : 345,
        "title" : "'java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset'",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/345"
      },
      {
        "number" : 343,
        "title" : "Error when using SharedSparkContext",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/343"
      },
      {
        "number" : 342,
        "title" : "Expected instance not static method org.scalatest.Assertions in DataFrame and DataSet SuitBase",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/342"
      },
      {
        "number" : 341,
        "title" : "Tests always fail if I have more than one test file extends `StreamingSuiteBase`",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/341"
      },
      {
        "number" : 340,
        "title" : "Need Junit 5 Support",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/340"
      },
      {
        "number" : 339,
        "title" : "Setting dedicated warehouse and metastore temp-directory doens't work",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/339"
      },
      {
        "number" : 338,
        "title" : "scala version ",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/338"
      },
      {
        "number" : 337,
        "title" : "Publish Spark 2.4.6 and Spark 2.4.7 versions of spark-testing-base",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/337"
      },
      {
        "number" : 335,
        "title" : "spark 2.4.0 cdh6.3.3 dependancies not compatible with spark-testing-base_2.11 2.4.0_0.14.0",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/335"
      },
      {
        "number" : 333,
        "title" : "Reduce log in unit test",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/333"
      },
      {
        "number" : 331,
        "title" : "Can't publish a local version",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/331"
      },
      {
        "number" : 329,
        "title" : "Publish Spark 3 Version?",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/329"
      },
      {
        "number" : 324,
        "title" : "SparkSession with DeltaLake related settings",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/324"
      },
      {
        "number" : 321,
        "title" : "Building Spark session fails with Hadoop 3 because of Hive Shims",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/321"
      },
      {
        "number" : 317,
        "title" : "Hortonworks Hive Warehouse Connector & Hadoop 3",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/317"
      },
      {
        "number" : 315,
        "title" : "NoSuchMethodError: org.scalatest.Assertions.assertionsHelper",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/315"
      },
      {
        "number" : 314,
        "title" : "New python release",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/314"
      },
      {
        "number" : 313,
        "title" : "Add support for Spark Streaming Kafka 0.10",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/313"
      },
      {
        "number" : 309,
        "title" : "Custom ArrayType Fields",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/309"
      },
      {
        "number" : 307,
        "title" : "A lot of exceptions during cleanup",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/307"
      },
      {
        "number" : 302,
        "title" : "Returning nulls for nullable types",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/302"
      },
      {
        "number" : 301,
        "title" : "Executors tab in Spark UI not working",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/301"
      },
      {
        "number" : 299,
        "title" : "Use HiveSupport in SparkSession and Azure storages",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/299"
      },
      {
        "number" : 297,
        "title" : "Option to turn Spark UI back on ?",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/297"
      },
      {
        "number" : 294,
        "title" : "Remove Spark pre-1.6 support from the build",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/294"
      },
      {
        "number" : 292,
        "title" : "Hive support is required to CREATE Hive TABLE (AS SELECT);;",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/292"
      },
      {
        "number" : 289,
        "title" : "py SparkTestingBase: incorrect code to instantiate SparkSession",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/289"
      },
      {
        "number" : 285,
        "title" : "Rows only in test missing after switch to Kryo",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/285"
      },
      {
        "number" : 283,
        "title" : "Spark tests failing in Jenkins but working in local",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/283"
      },
      {
        "number" : 279,
        "title" : "Explore if pytest-xdist can work with spark tests",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/279"
      },
      {
        "number" : 278,
        "title" : "Use same Hadoop version as spark target?",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/278"
      },
      {
        "number" : 272,
        "title" : "More info when comparing different DataFrame/DataSet",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/272"
      },
      {
        "number" : 269,
        "title" : "Misleading error when comparing resutls",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/269"
      },
      {
        "number" : 268,
        "title" : "Spark 2.2.2 support",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/268"
      },
      {
        "number" : 267,
        "title" : "Temporary warehouse on local is not a valid hdfs path (SharedSparkContext)",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/267"
      },
      {
        "number" : 265,
        "title" : "Dependency issues with  spark-testing-base_2.10, version 1.6.3_0.10.0",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/265"
      },
      {
        "number" : 261,
        "title" : "Global spark configuration changes are not maintained during Maven tests",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/261"
      },
      {
        "number" : 260,
        "title" : "hadoop-hdfs: NoSuchMethodError - org.apache.hadoop.tracing.SpanReceiverHost.getInstance",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/260"
      },
      {
        "number" : 259,
        "title" : "Exposing Builder object created in DataFrameSuiteBaseLike",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/259"
      },
      {
        "number" : 247,
        "title" : "Comparing RDD of org.apache.spark.sql.Row containing byte[] as values fails for the same provided RDD",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/247"
      },
      {
        "number" : 243,
        "title" : "assertDataFrameApproximateEquals failed",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/243"
      },
      {
        "number" : 242,
        "title" : "SqlContext test cases not running..",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/242"
      },
      {
        "number" : 240,
        "title" : "Parallel Execution",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/240"
      },
      {
        "number" : 239,
        "title" : "Multiple Coverage Highlighting in the build",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/239"
      },
      {
        "number" : 237,
        "title" : "RDDComparisons could use a more descriptive error",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/237"
      },
      {
        "number" : 236,
        "title" : "assertDataFrameEquals message says that 2 schemas are different after printing out the exact same schemas",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/236"
      },
      {
        "number" : 235,
        "title" : "PySpark: SparkTestingBaseTestCase.assertRDDEquals is not an assertion",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/235"
      },
      {
        "number" : 234,
        "title" : "Hive support lost when upgrading from spark-testing-base 2.2.0_0.9.0 to 2.3.0_0.9.0",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/234"
      },
      {
        "number" : 230,
        "title" : "Length & schema checks report expected as actual",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/230"
      },
      {
        "number" : 226,
        "title" : "python - unnecessary dependency on pytest in setup.py?",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/226"
      },
      {
        "number" : 225,
        "title" : "release and publish 2.2.1_0.8.*",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/225"
      },
      {
        "number" : 224,
        "title" : "hive exception with Spark1.6",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/224"
      },
      {
        "number" : 221,
        "title" : "DataframeSuiteBase approxEquals(): need support with specified tolerence type",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/221"
      },
      {
        "number" : 219,
        "title" : " java.io.IOException: failure to login: No LoginModules configured for hadoop_simple",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/219"
      },
      {
        "number" : 217,
        "title" : "StructuredStreamingBase missing inside 2.2.0_0.8.0",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/217"
      },
      {
        "number" : 214,
        "title" : "approxEquals with relative tolerance",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/214"
      },
      {
        "number" : 212,
        "title" : "eclipse integration is not working as expected for spark 1.6.x",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/212"
      },
      {
        "number" : 211,
        "title" : "DataframeGenerator : Ensure minimum number of distinct values",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/211"
      },
      {
        "number" : 205,
        "title" : "Incompability with hbase-testing-util",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/205"
      },
      {
        "number" : 198,
        "title" : "Just make hive explicitly required",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/198"
      },
      {
        "number" : 191,
        "title" : "Guava Stopwatch bug",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/191"
      },
      {
        "number" : 186,
        "title" : "Remove EvilSessionTools / SparkSession from DataFrameSuiteBase",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/186"
      },
      {
        "number" : 180,
        "title" : "NPE when using SparkSession from DataFrameSuiteBase",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/180"
      },
      {
        "number" : 179,
        "title" : "DataFrame comparision with different schema metadata error message is confusing",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/179"
      },
      {
        "number" : 171,
        "title" : "Minicluster throws NPE on shutdown",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/171"
      },
      {
        "number" : 168,
        "title" : "Streaming testOperation() for more than 2 streams",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/168"
      },
      {
        "number" : 167,
        "title" : "Error while instantiating 'org.apache.spark.sql.hive.HiveSessionState'  on DataFrameSuiteBase",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/167"
      },
      {
        "number" : 164,
        "title" : "Testing Structured Streaming Use Cases",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/164"
      },
      {
        "number" : 163,
        "title" : "java.lang.IncompatibleClassChangeError",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/163"
      },
      {
        "number" : 161,
        "title" : "Plan to decouple with UnitTest?",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/161"
      },
      {
        "number" : 142,
        "title" : "StreamingSuitBase test method ERROR",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/142"
      },
      {
        "number" : 136,
        "title" : "Checkpoint directory has not been set in the SparkContext",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/136"
      },
      {
        "number" : 134,
        "title" : "specs2 support ?",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/134"
      },
      {
        "number" : 129,
        "title" : "Trying to use RDDComaprisons in scala 2.11 and spark 1.6",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/129"
      },
      {
        "number" : 128,
        "title" : "Support for ScalaTest maven plugin.",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/128"
      },
      {
        "number" : 124,
        "title" : "Switch hive dependency to non-test dep and add a note for people that want to exclude it.",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/124"
      },
      {
        "number" : 123,
        "title" : "Created shared version of DataFrameSuiteBase",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/123"
      },
      {
        "number" : 117,
        "title" : "JavaRDDComparisons on RDDs with Tuples with Decimals",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/117"
      },
      {
        "number" : 115,
        "title" : "Add test for TimeStamp generator",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/115"
      },
      {
        "number" : 108,
        "title" : "Is it possible to use DataFrameSuiteBase with FunSpec(scalatest)?",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/108"
      },
      {
        "number" : 101,
        "title" : "Not allowed to query with all of partition Keys(cassandra) with Equals",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/101"
      },
      {
        "number" : 93,
        "title" : "Point Out Spark 1.6's Hive Dependency",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/93"
      },
      {
        "number" : 75,
        "title" : "Run Streaming Tests Distributively ",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/75"
      },
      {
        "number" : 73,
        "title" : "Nice to have  more types in  DataFrameSuiteBaseLike.approxEquals",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/73"
      },
      {
        "number" : 63,
        "title" : "how to test foreachRDD in python",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/63"
      },
      {
        "number" : 58,
        "title" : "Create project page at github.io",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/58"
      },
      {
        "number" : 47,
        "title" : "Add a code of conduct",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/47"
      },
      {
        "number" : 37,
        "title" : "More documentation!",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/37"
      },
      {
        "number" : 33,
        "title" : "Serialization for Mockito improvement",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/33"
      },
      {
        "number" : 23,
        "title" : "Add Hypothesis for Python",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/23"
      },
      {
        "number" : 15,
        "title" : "Add support for quickcheck DStreams",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/15"
      },
      {
        "number" : 14,
        "title" : "Implement quickcheck type support for Java & Python",
        "url" : "https://github.com/holdenk/spark-testing-base/issues/14"
      }
    ],
    "scalaPercentage" : null
  },
  "settings" : {
    "defaultStableVersion" : true,
    "defaultArtifact" : "spark-testing-base",
    "strictVersions" : false,
    "customScalaDoc" : null,
    "documentationLinks" : [
    ],
    "deprecated" : false,
    "contributorsWanted" : false,
    "artifactDeprecations" : [
    ],
    "cliArtifacts" : [
    ],
    "category" : "testing",
    "beginnerIssuesLabel" : null
  }
}